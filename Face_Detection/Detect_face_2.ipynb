{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ebf6e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepface'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Single line emotion detection (very fast)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepface'"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Single line emotion detection (very fast)\n",
    "def quick_emotion_detect(frame):\n",
    "    try:\n",
    "        result = DeepFace.analyze(frame, actions=['emotion'], \n",
    "                                enforce_detection=False, silent=True)\n",
    "        return result[0]['dominant_emotion']\n",
    "    except:\n",
    "        return 'neutral'\n",
    "\n",
    "# Usage\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    emotion = quick_emotion_detect(frame)\n",
    "    cv2.putText(frame, emotion, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('Quick Emotion', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9807fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://vision.googleapis.com/v1/images:annotate?key=AIzaSyBysttIKzTZgxk64l1VJOE4k7Om4vGELhU",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m emotion \u001b[38;5;241m=\u001b[39m \u001b[43mgcv_emotion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, emotion, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m),\n\u001b[0;32m     42\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     43\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGCV emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m, in \u001b[0;36mgcv_emotion\u001b[1;34m(frame_bgr)\u001b[0m\n\u001b[0;32m     10\u001b[0m body \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: img_b64},\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFACE_DETECTION\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m     14\u001b[0m     }]\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     16\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(ENDPOINT, json\u001b[38;5;241m=\u001b[39mbody, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m faces \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaceAnnotations\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m faces:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1023\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://vision.googleapis.com/v1/images:annotate?key=AIzaSyBysttIKzTZgxk64l1VJOE4k7Om4vGELhU"
     ]
    }
   ],
   "source": [
    "import cv2, base64, requests, json, os\n",
    "\n",
    "API_KEY  = \"AIzaSyBysttIKzTZgxk64l1VJOE4k7Om4vGELhU\"                         # from Google Cloud Console\n",
    "ENDPOINT = f\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\"\n",
    "\n",
    "def gcv_emotion(frame_bgr):\n",
    "    _, jpg = cv2.imencode(\".jpg\", frame_bgr, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "    img_b64 = base64.b64encode(jpg).decode()\n",
    "\n",
    "    body = {\n",
    "        \"requests\": [{\n",
    "            \"image\": {\"content\": img_b64},\n",
    "            \"features\": [{\"type\": \"FACE_DETECTION\"}]\n",
    "        }]\n",
    "    }\n",
    "    r = requests.post(ENDPOINT, json=body, timeout=2)\n",
    "    r.raise_for_status()\n",
    "    faces = r.json()[\"responses\"][0].get(\"faceAnnotations\", [])\n",
    "    if not faces:\n",
    "        return \"no-face\"\n",
    "\n",
    "    emo_map = {\n",
    "        \"VERY_LIKELY\": 5, \"LIKELY\": 4, \"POSSIBLE\": 3,\n",
    "        \"UNLIKELY\": 2, \"VERY_UNLIKELY\": 1\n",
    "    }\n",
    "    best = max(\n",
    "        (\"joyLikelihood\",   \"happy\"),\n",
    "        (\"angerLikelihood\", \"angry\"),\n",
    "        (\"sorrowLikelihood\",\"sad\"),\n",
    "        (\"surpriseLikelihood\",\"surprised\"),\n",
    "        key=lambda k: emo_map[faces[0][k[0]]]\n",
    "    )\n",
    "    return best[1]\n",
    "\n",
    "cap = cv2.VideoCapture(0)           # your webcam\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "    emotion = gcv_emotion(frame)\n",
    "    cv2.putText(frame, emotion, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"GCV emotion\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release(); cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a03182",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'moviepy.editor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FER\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the emotion detector\u001b[39;00m\n\u001b[0;32m      5\u001b[0m detector \u001b[38;5;241m=\u001b[39m FER(mtcnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# mtcnn=True for better face detection\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fer\\__init__.py:27\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/python3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# SOFTWARE.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Video\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FER\n\u001b[0;32m     30\u001b[0m log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fer\\classes.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meditor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Union\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'moviepy.editor'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "\n",
    "# Initialize the emotion detector\n",
    "detector = FER(mtcnn=True)  # mtcnn=True for better face detection\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Starting emotion detection. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Detect emotions in the frame\n",
    "    emotions = detector.detect_emotions(frame)\n",
    "    \n",
    "    # If faces are detected\n",
    "    if emotions:\n",
    "        # Get the first face's emotions\n",
    "        emotion_scores = emotions[0][\"emotions\"]\n",
    "        \n",
    "        # Find the dominant emotion\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        # Draw bounding box around face\n",
    "        (x, y, w, h) = emotions[0][\"box\"]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display emotion text with confidence score\n",
    "        text = f\"{dominant_emotion.upper()}: {confidence:.2f}\"\n",
    "        cv2.putText(frame, text, (x, y - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        # Optional: Display all emotion scores\n",
    "        y_offset = y + h + 25\n",
    "        for emotion, score in emotion_scores.items():\n",
    "            if score > 0.1:  # Only show emotions with >10% confidence\n",
    "                emotion_text = f\"{emotion}: {score:.2f}\"\n",
    "                cv2.putText(frame, emotion_text, (x, y_offset), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                y_offset += 20\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow('Real-time Emotion Detection', frame)\n",
    "    \n",
    "    # Break on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Emotion detection stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7babd0c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     10\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 11\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     14\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x, y), (x\u001b[38;5;241m+\u001b[39mw, y\u001b[38;5;241m+\u001b[39mh), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Simple emotion detection with OpenCV (basic implementation)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, \"Face Detected\", (x, y-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Face Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629e7749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 AI Interview Emotion Analysis\n",
      "Press 'q' to end interview and generate report\n",
      "Press 's' to start/restart interview session\n",
      "🎥 Interview session started: 20250904_233603\n",
      "🎥 Interview session started: 20250904_233603\n",
      "💾 Data saved: interview_20250904_233603_summary.json & interview_20250904_233603_detailed.csv\n",
      "📊 Interview completed. Duration: 13.3s\n",
      "\n",
      "==================================================\n",
      "📊 INTERVIEW ANALYSIS COMPLETE\n",
      "==================================================\n",
      "Overall Assessment: Fair - Candidate experienced moderate emotional fluctuations\n",
      "Emotional Stability: 97.2%\n",
      "Dominant Emotion: Neutral\n",
      "Total Duration: 13.3s\n",
      "\n",
      "🎭 Emotional Breakdown:\n",
      "  Neutral: 93.15%\n",
      "  Happy: 0.6%\n",
      "  Fear: 1.13%\n",
      "  Angry: 4.67%\n",
      "\n",
      "⚠️ Stress Indicators:\n",
      "  Anger Moments: 4\n",
      "  Anxiety Moments: 2\n",
      "\n",
      "🚨 Critical Moments (6):\n",
      "  8.6s - Candidate appeared anxious or fearful\n",
      "  8.7s - Candidate appeared anxious or fearful\n",
      "  11.8s - Candidate displayed anger or frustration\n",
      "\n",
      "💡 Recommendations:\n",
      "  • Consider incorporating more engaging or positive discussion topics\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "\n",
    "class InterviewEmotionAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'critical_moments': []\n",
    "        }\n",
    "        self.emotion_history = deque(maxlen=30)  # Last 30 detections for smoothing\n",
    "        \n",
    "    def start_interview(self):\n",
    "        \"\"\"Start interview session\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Interview session started: {self.current_session['session_id']}\")\n",
    "        \n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"Analyze single frame and return emotion data\"\"\"\n",
    "        if self.start_time is None:\n",
    "            self.start_interview()\n",
    "            \n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        # Detect emotions\n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if emotions:\n",
    "            emotion_scores = emotions[0][\"emotions\"]\n",
    "            dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "            confidence = emotion_scores[dominant_emotion]\n",
    "            box = emotions[0][\"box\"]\n",
    "            \n",
    "            # Add to history for smoothing\n",
    "            self.emotion_history.append(dominant_emotion)\n",
    "            \n",
    "            # Get smoothed emotion (most common in last 10 detections)\n",
    "            if len(self.emotion_history) >= 10:\n",
    "                emotion_counts = {}\n",
    "                recent_emotions = list(self.emotion_history)[-10:]\n",
    "                for emotion in recent_emotions:\n",
    "                    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "                smoothed_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0]\n",
    "            else:\n",
    "                smoothed_emotion = dominant_emotion\n",
    "            \n",
    "            # Log emotion data\n",
    "            emotion_entry = {\n",
    "                'timestamp': current_time.isoformat(),\n",
    "                'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "                'dominant_emotion': smoothed_emotion,\n",
    "                'confidence': round(confidence, 3),\n",
    "                'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "                'face_box': box\n",
    "            }\n",
    "            \n",
    "            self.emotion_log.append(emotion_entry)\n",
    "            self.current_session['emotion_timeline'].append(emotion_entry)\n",
    "            \n",
    "            # Detect critical moments\n",
    "            self.detect_critical_moments(emotion_entry, elapsed_seconds)\n",
    "            \n",
    "            return {\n",
    "                'emotion': smoothed_emotion,\n",
    "                'confidence': confidence,\n",
    "                'box': box,\n",
    "                'all_emotions': emotion_scores,\n",
    "                'elapsed_time': elapsed_seconds\n",
    "            }\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def detect_critical_moments(self, emotion_entry, elapsed_seconds):\n",
    "        \"\"\"Detect significant emotional moments\"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        \n",
    "        # Define critical moments\n",
    "        if emotion == 'sad' and confidence > 0.3:\n",
    "            self.current_session['critical_moments'].append({\n",
    "                'type': 'high_sadness',\n",
    "                'timestamp': emotion_entry['timestamp'],\n",
    "                'elapsed_seconds': elapsed_seconds,\n",
    "                'emotion': emotion,\n",
    "                'confidence': confidence,\n",
    "                'description': 'Candidate showed signs of distress or sadness'\n",
    "            })\n",
    "        \n",
    "        elif emotion == 'angry' and confidence > 0.4:\n",
    "            self.current_session['critical_moments'].append({\n",
    "                'type': 'anger_detected',\n",
    "                'timestamp': emotion_entry['timestamp'],\n",
    "                'elapsed_seconds': elapsed_seconds,\n",
    "                'emotion': emotion,\n",
    "                'confidence': confidence,\n",
    "                'description': 'Candidate displayed anger or frustration'\n",
    "            })\n",
    "            \n",
    "        elif emotion == 'fear' and confidence > 0.3:\n",
    "            self.current_session['critical_moments'].append({\n",
    "                'type': 'anxiety_detected',\n",
    "                'timestamp': emotion_entry['timestamp'],\n",
    "                'elapsed_seconds': elapsed_seconds,\n",
    "                'emotion': emotion,\n",
    "                'confidence': confidence,\n",
    "                'description': 'Candidate appeared anxious or fearful'\n",
    "            })\n",
    "    \n",
    "    def end_interview(self):\n",
    "        \"\"\"End interview and generate summary\"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        total_duration = (end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round(total_duration, 2)\n",
    "        \n",
    "        # Generate emotion summary\n",
    "        self.generate_emotion_summary()\n",
    "        \n",
    "        # Save session data\n",
    "        self.save_session_data()\n",
    "        \n",
    "        print(f\"📊 Interview completed. Duration: {total_duration:.1f}s\")\n",
    "        return self.current_session\n",
    "    \n",
    "    def generate_emotion_summary(self):\n",
    "        \"\"\"Generate comprehensive emotion analysis\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "            \n",
    "        # Calculate time spent in each emotion\n",
    "        emotion_durations = defaultdict(float)\n",
    "        emotion_counts = defaultdict(int)\n",
    "        \n",
    "        for i, entry in enumerate(self.emotion_log):\n",
    "            emotion = entry['dominant_emotion']\n",
    "            emotion_counts[emotion] += 1\n",
    "            \n",
    "            # Calculate duration (approximate)\n",
    "            if i < len(self.emotion_log) - 1:\n",
    "                duration = self.emotion_log[i+1]['elapsed_seconds'] - entry['elapsed_seconds']\n",
    "                emotion_durations[emotion] += duration\n",
    "        \n",
    "        total_duration = self.current_session['total_duration']\n",
    "        \n",
    "        # Calculate percentages\n",
    "        emotion_percentages = {}\n",
    "        for emotion, duration in emotion_durations.items():\n",
    "            percentage = (duration / total_duration) * 100 if total_duration > 0 else 0\n",
    "            emotion_percentages[emotion] = round(percentage, 2)\n",
    "        \n",
    "        # Emotional stability analysis\n",
    "        emotion_changes = 0\n",
    "        if len(self.emotion_log) > 1:\n",
    "            for i in range(1, len(self.emotion_log)):\n",
    "                if self.emotion_log[i]['dominant_emotion'] != self.emotion_log[i-1]['dominant_emotion']:\n",
    "                    emotion_changes += 1\n",
    "        \n",
    "        stability_score = max(0, 100 - (emotion_changes / len(self.emotion_log) * 100)) if self.emotion_log else 0\n",
    "        \n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotion_durations': dict(emotion_durations),\n",
    "            'emotion_counts': dict(emotion_counts),\n",
    "            'total_emotion_changes': emotion_changes,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages.items(), key=lambda x: x[1])[0] if emotion_percentages else 'neutral',\n",
    "            'stress_indicators': {\n",
    "                'high_sadness_moments': len([m for m in self.current_session['critical_moments'] if m['type'] == 'high_sadness']),\n",
    "                'anger_moments': len([m for m in self.current_session['critical_moments'] if m['type'] == 'anger_detected']),\n",
    "                'anxiety_moments': len([m for m in self.current_session['critical_moments'] if m['type'] == 'anxiety_detected'])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def save_session_data(self):\n",
    "        \"\"\"Save session data to files\"\"\"\n",
    "        session_id = self.current_session['session_id']\n",
    "        \n",
    "        # Save JSON summary\n",
    "        with open(f'interview_{session_id}_summary.json', 'w') as f:\n",
    "            json.dump(self.current_session, f, indent=2, default=str)\n",
    "        \n",
    "        # Save detailed CSV\n",
    "        if self.emotion_log:\n",
    "            df = pd.DataFrame(self.emotion_log)\n",
    "            df.to_csv(f'interview_{session_id}_detailed.csv', index=False)\n",
    "        \n",
    "        print(f\"💾 Data saved: interview_{session_id}_summary.json & interview_{session_id}_detailed.csv\")\n",
    "    \n",
    "    def generate_dashboard_insights(self):\n",
    "        \"\"\"Generate insights for dashboard display\"\"\"\n",
    "        if not self.current_session['emotion_summary']:\n",
    "            return {}\n",
    "            \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        \n",
    "        insights = {\n",
    "            'overall_assessment': self.get_overall_assessment(),\n",
    "            'key_metrics': {\n",
    "                'emotional_stability': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "                'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "                'total_duration': f\"{self.current_session['total_duration']:.1f}s\",\n",
    "                'emotion_changes': summary['total_emotion_changes']\n",
    "            },\n",
    "            'emotional_breakdown': summary['emotion_percentages'],\n",
    "            'stress_indicators': summary['stress_indicators'],\n",
    "            'critical_moments': self.current_session['critical_moments'],\n",
    "            'recommendations': self.generate_recommendations()\n",
    "        }\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def get_overall_assessment(self):\n",
    "        \"\"\"Generate overall emotional assessment\"\"\"\n",
    "        summary = self.current_session['emotion_summary']\n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary['stress_indicators'].values())\n",
    "        \n",
    "        if stability >= 80 and stress_total <= 2:\n",
    "            return \"Excellent - Candidate remained calm and composed throughout the interview\"\n",
    "        elif stability >= 60 and stress_total <= 5:\n",
    "            return \"Good - Candidate showed generally stable emotions with minor stress moments\"\n",
    "        elif stability >= 40 and stress_total <= 8:\n",
    "            return \"Fair - Candidate experienced moderate emotional fluctuations\"\n",
    "        else:\n",
    "            return \"Needs Attention - Candidate showed significant emotional instability or stress\"\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        summary = self.current_session['emotion_summary']\n",
    "        recommendations = []\n",
    "        \n",
    "        if summary['stress_indicators']['high_sadness_moments'] > 3:\n",
    "            recommendations.append(\"Consider providing more encouragement and positive feedback during interviews\")\n",
    "        \n",
    "        if summary['stress_indicators']['anxiety_moments'] > 2:\n",
    "            recommendations.append(\"Create a more relaxed interview environment to reduce candidate anxiety\")\n",
    "        \n",
    "        if summary['emotional_stability_score'] < 50:\n",
    "            recommendations.append(\"Candidate may benefit from interview coaching or stress management techniques\")\n",
    "        \n",
    "        if summary['emotion_percentages'].get('happy', 0) < 20:\n",
    "            recommendations.append(\"Consider incorporating more engaging or positive discussion topics\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Enhanced main detection loop\n",
    "def run_interview_analysis():\n",
    "    analyzer = InterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera\")\n",
    "        return\n",
    "    \n",
    "    print(\"🎤 AI Interview Emotion Analysis\")\n",
    "    print(\"Press 'q' to end interview and generate report\")\n",
    "    print(\"Press 's' to start/restart interview session\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Start interview\n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        # Analyze frame if interview is active\n",
    "        emotion_data = None\n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "        \n",
    "        # Display results\n",
    "        if emotion_data:\n",
    "            x, y, w, h = emotion_data['box']\n",
    "            emotion = emotion_data['emotion']\n",
    "            confidence = emotion_data['confidence']\n",
    "            elapsed_time = emotion_data['elapsed_time']\n",
    "            \n",
    "            # Color coding for emotions\n",
    "            color_map = {\n",
    "                'happy': (0, 255, 0),      # Green\n",
    "                'sad': (0, 0, 255),        # Red\n",
    "                'angry': (0, 0, 139),      # Dark Red\n",
    "                'fear': (128, 0, 128),     # Purple\n",
    "                'surprise': (255, 255, 0), # Yellow\n",
    "                'neutral': (255, 255, 255) # White\n",
    "            }\n",
    "            \n",
    "            color = color_map.get(emotion, (255, 255, 255))\n",
    "            \n",
    "            # Draw bounding box and emotion\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{emotion.upper()}: {confidence:.2f}\", \n",
    "                       (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Display elapsed time\n",
    "            cv2.putText(frame, f\"Time: {elapsed_time:.1f}s\", \n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            # Show emotional breakdown\n",
    "            y_offset = 60\n",
    "            for emotion_name, score in emotion_data['all_emotions'].items():\n",
    "                if score > 0.1:\n",
    "                    text = f\"{emotion_name}: {score:.2f}\"\n",
    "                    cv2.putText(frame, text, (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)\n",
    "                    y_offset += 20\n",
    "        \n",
    "        # Status indicator\n",
    "        status = \"RECORDING\" if interview_started else \"READY - Press 's' to start\"\n",
    "        status_color = (0, 0, 255) if interview_started else (0, 255, 0)\n",
    "        cv2.putText(frame, status, (10, frame.shape[0] - 20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)\n",
    "        \n",
    "        cv2.imshow('AI Interview Analysis', frame)\n",
    "        \n",
    "        # End interview\n",
    "        if key == ord('q'):\n",
    "            if interview_started:\n",
    "                session_data = analyzer.end_interview()\n",
    "                \n",
    "                # Generate dashboard insights\n",
    "                insights = analyzer.generate_dashboard_insights()\n",
    "                \n",
    "                # Display summary\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"📊 INTERVIEW ANALYSIS COMPLETE\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"Overall Assessment: {insights['overall_assessment']}\")\n",
    "                print(f\"Emotional Stability: {insights['key_metrics']['emotional_stability']}\")\n",
    "                print(f\"Dominant Emotion: {insights['key_metrics']['dominant_emotion']}\")\n",
    "                print(f\"Total Duration: {insights['key_metrics']['total_duration']}\")\n",
    "                \n",
    "                print(\"\\n🎭 Emotional Breakdown:\")\n",
    "                for emotion, percentage in insights['emotional_breakdown'].items():\n",
    "                    print(f\"  {emotion.title()}: {percentage}%\")\n",
    "                \n",
    "                print(f\"\\n⚠️ Stress Indicators:\")\n",
    "                for indicator, count in insights['stress_indicators'].items():\n",
    "                    if count > 0:\n",
    "                        print(f\"  {indicator.replace('_', ' ').title()}: {count}\")\n",
    "                \n",
    "                if insights['critical_moments']:\n",
    "                    print(f\"\\n🚨 Critical Moments ({len(insights['critical_moments'])}):\")\n",
    "                    for moment in insights['critical_moments'][:3]:  # Show first 3\n",
    "                        print(f\"  {moment['elapsed_seconds']:.1f}s - {moment['description']}\")\n",
    "                \n",
    "                if insights['recommendations']:\n",
    "                    print(f\"\\n💡 Recommendations:\")\n",
    "                    for rec in insights['recommendations']:\n",
    "                        print(f\"  • {rec}\")\n",
    "                \n",
    "                print(\"=\"*50)\n",
    "                \n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_interview_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029a4dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 AI Interview Emotion Analysis (Improved)\n",
      "Press 's' to START or RESTART the interview.\n",
      "Press 'q' to END interview and generate the report.\n",
      "🎥 Interview session started: 20250905_011812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Data saved to: interview_20250905_011812_summary.json & _detailed.csv\n",
      "\n",
      "📊 Interview completed. Duration: 7.2s\n",
      "\n",
      "============================================================\n",
      "📊 FINAL INTERVIEW REPORT\n",
      "============================================================\n",
      "Overall Assessment: Good: Candidate showed solid emotional control with minor signs of stress.\n",
      "Emotional Stability: 94.7%\n",
      "Dominant Emotion Profile: Angry\n",
      "\n",
      "🎭 Emotional Breakdown (% of time):\n",
      "  - Angry     : 60.0%\n",
      "  - Happy     : 26.67%\n",
      "  - Sad       : 13.33%\n",
      "\n",
      "🚨 Critical Moments Summary:\n",
      "  - High Stress: 5 instance(s)\n",
      "\n",
      "💡 Recommendations:\n",
      "  • Probe gently into topics that may have caused stress to understand the context.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class InterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes facial emotions in real-time during an interview, providing insights\n",
    "    into the candidate's emotional state, stability, and critical moments.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Use deques for efficient, fixed-size history tracking\n",
    "        self.emotion_history = deque(maxlen=15)  # Smoothed over last 15 frames\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'critical_moments': []\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        # Reset all tracking variables for a clean start\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Interview session started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Analyzes a single video frame for emotions, logs the data, and detects\n",
    "        critical moments. Returns processed data for display.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None  # Do not start automatically, wait for user input\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        # Primary face's emotion data\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        # Get a smoothed emotion to reduce rapid fluctuations\n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "\n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'raw_emotion': dominant_emotion,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "            'face_box': primary_face[\"box\"]\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        self.current_session['emotion_timeline'].append(emotion_entry)\n",
    "        \n",
    "        self.detect_critical_moments(emotion_entry)\n",
    "        \n",
    "        return emotion_entry\n",
    "\n",
    "    def detect_critical_moments(self, emotion_entry):\n",
    "        \"\"\"\n",
    "        Identifies and flags moments of significant emotional shifts, stress,\n",
    "        or disengagement based on refined heuristics.\n",
    "        \"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        all_emotions = emotion_entry['all_emotions']\n",
    "        elapsed_seconds = emotion_entry['elapsed_seconds']\n",
    "\n",
    "        # --- Refined Critical Moment Detection ---\n",
    "\n",
    "        # 1. High Sadness or Fear (Strong negative indicators)\n",
    "        if (emotion == 'sad' and confidence > 0.35) or (emotion == 'fear' and confidence > 0.4):\n",
    "            self._log_critical_moment('high_stress', emotion_entry, \n",
    "                                      f\"Candidate showed strong signs of {emotion}.\")\n",
    "\n",
    "        # 2. Prolonged Neutrality (Potential Disengagement/Dullness)\n",
    "        if emotion == 'neutral':\n",
    "            self.neutral_streak += 1\n",
    "            if self.neutral_streak == 30: # Flag after ~30 consecutive neutral frames\n",
    "                self._log_critical_moment('prolonged_neutrality', emotion_entry,\n",
    "                                          \"Candidate showed a prolonged neutral expression (potential disinterest).\")\n",
    "        else:\n",
    "            self.neutral_streak = 0 # Reset streak if not neutral\n",
    "\n",
    "        # 3. Low Confidence / Uncertainty (Improved Logic)\n",
    "        # This is now much stricter to avoid false positives from talking.\n",
    "        # It looks for a sustained state of ambiguity where the model is not confident\n",
    "        # and there is a mix of negative emotions without any clear dominant one.\n",
    "        is_ambiguous_negative = (\n",
    "            all_emotions.get('neutral', 0) > 0.20 and\n",
    "            (all_emotions.get('sad', 0) + all_emotions.get('fear', 0)) > 0.25 and\n",
    "            all_emotions.get('happy', 0) < 0.10 # Ensure no happiness\n",
    "        )\n",
    "\n",
    "        if confidence < 0.45 and is_ambiguous_negative:\n",
    "            self.low_confidence_streak += 1\n",
    "            if self.low_confidence_streak == 15: # Flag only after 15 frames of this state\n",
    "                self._log_critical_moment('low_confidence', emotion_entry, \n",
    "                                          \"Candidate appeared uncertain or less confident.\")\n",
    "        else:\n",
    "            self.low_confidence_streak = 0 # Reset streak\n",
    "\n",
    "    def _log_critical_moment(self, type, entry, description):\n",
    "        \"\"\"Helper to append a critical moment to the session log.\"\"\"\n",
    "        self.current_session['critical_moments'].append({\n",
    "            'type': type,\n",
    "            'timestamp': entry['timestamp'],\n",
    "            'elapsed_seconds': entry['elapsed_seconds'],\n",
    "            'emotion': entry['dominant_emotion'],\n",
    "            'confidence': entry['confidence'],\n",
    "            'description': description\n",
    "        })\n",
    "        \n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates all reports.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return None\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_emotion_summary()\n",
    "        self.save_session_data()\n",
    "        \n",
    "        print(f\"\\n📊 Interview completed. Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "        return self.current_session\n",
    "\n",
    "    def generate_emotion_summary(self):\n",
    "        \"\"\"Calculates and stores aggregate emotion statistics for the session.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        # Calculate percentage of time spent in each emotion\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "\n",
    "        # Calculate emotional stability\n",
    "        emotion_changes = (df['dominant_emotion'].shift() != df['dominant_emotion']).sum()\n",
    "        stability_score = max(0, 100 - (emotion_changes / len(df) * 100))\n",
    "        \n",
    "        # Summarize critical moments by type\n",
    "        critical_moment_counts = defaultdict(int)\n",
    "        for moment in self.current_session['critical_moments']:\n",
    "            critical_moment_counts[moment['type']] += 1\n",
    "\n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages, key=emotion_percentages.get, default='neutral'),\n",
    "            'critical_moment_counts': dict(critical_moment_counts)\n",
    "        }\n",
    "\n",
    "    def save_session_data(self):\n",
    "        \"\"\"Saves a detailed CSV and a summary JSON file for the session.\"\"\"\n",
    "        session_id = self.current_session['session_id']\n",
    "        # Save JSON summary\n",
    "        with open(f'interview_{session_id}_summary.json', 'w') as f:\n",
    "            json.dump(self.current_session, f, indent=4, default=str)\n",
    "        # Save detailed CSV\n",
    "        if self.emotion_log:\n",
    "            pd.DataFrame(self.emotion_log).to_csv(f'interview_{session_id}_detailed.csv', index=False)\n",
    "        print(f\"💾 Data saved to: interview_{session_id}_summary.json & _detailed.csv\")\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Generates a structured, human-readable report from the session data.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No summary available. Was the interview run and ended properly?\"}\n",
    "        \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary.get('critical_moment_counts', {}).values())\n",
    "\n",
    "        # Determine overall assessment\n",
    "        if stability >= 75 and stress_total <= 2:\n",
    "            assessment = \"Excellent: Candidate was highly composed, confident, and emotionally stable.\"\n",
    "        elif stability >= 55 and stress_total <= 5:\n",
    "            assessment = \"Good: Candidate showed solid emotional control with minor signs of stress.\"\n",
    "        elif stability >= 35 or summary.get('critical_moment_counts', {}).get('low_confidence', 0) > 0:\n",
    "            assessment = \"Fair: Candidate experienced some emotional fluctuations or moments of uncertainty.\"\n",
    "        else:\n",
    "            assessment = \"Needs Attention: Candidate showed significant emotional instability or stress.\"\n",
    "\n",
    "        # Generate actionable recommendations\n",
    "        recommendations = []\n",
    "        counts = summary.get('critical_moment_counts', {})\n",
    "        if counts.get('high_stress', 0) > 1:\n",
    "            recommendations.append(\"Probe gently into topics that may have caused stress to understand the context.\")\n",
    "        if counts.get('low_confidence', 0) > 0:\n",
    "            recommendations.append(\"Candidate showed signs of uncertainty; consider asking questions to boost their comfort.\")\n",
    "        if counts.get('prolonged_neutrality', 0) > 0:\n",
    "            recommendations.append(\"Candidate may seem disengaged; try using more dynamic questions to spark interest.\")\n",
    "        if not recommendations:\n",
    "            recommendations.append(\"Candidate performed well emotionally. No specific concerns noted.\")\n",
    "            \n",
    "        return {\n",
    "            'assessment': assessment,\n",
    "            'stability_score': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "            'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "            'emotion_breakdown': summary['emotion_percentages'],\n",
    "            'critical_moments_summary': counts,\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "\n",
    "\n",
    "def run_interview_analysis():\n",
    "    \"\"\"Main loop to capture video, run analysis, and display results.\"\"\"\n",
    "    analyzer = InterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 AI Interview Emotion Analysis (Improved)\")\n",
    "    print(\"Press 's' to START or RESTART the interview.\")\n",
    "    print(\"Press 'q' to END interview and generate the report.\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # Display final report in console\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"📊 FINAL INTERVIEW REPORT\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"Overall Assessment: {report['assessment']}\")\n",
    "                print(f\"Emotional Stability: {report['stability_score']}\")\n",
    "                print(f\"Dominant Emotion Profile: {report['dominant_emotion']}\")\n",
    "                \n",
    "                print(\"\\n🎭 Emotional Breakdown (% of time):\")\n",
    "                for emotion, pct in sorted(report['emotion_breakdown'].items()):\n",
    "                    print(f\"  - {emotion.title():<10}: {pct}%\")\n",
    "                \n",
    "                if report['critical_moments_summary']:\n",
    "                    print(\"\\n🚨 Critical Moments Summary:\")\n",
    "                    for type, count in report['critical_moments_summary'].items():\n",
    "                        print(f\"  - {type.replace('_', ' ').title()}: {count} instance(s)\")\n",
    "\n",
    "                print(\"\\n💡 Recommendations:\")\n",
    "                for rec in report['recommendations']:\n",
    "                    print(f\"  • {rec}\")\n",
    "                print(\"=\"*60)\n",
    "            break\n",
    "        \n",
    "        # Analyze and display frame data if interview is active\n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            if emotion_data:\n",
    "                x, y, w, h = emotion_data['face_box']\n",
    "                emotion = emotion_data['dominant_emotion']\n",
    "                confidence = emotion_data['confidence']\n",
    "                \n",
    "                color_map = {'happy': (0, 255, 0), 'sad': (255, 100, 0), 'angry': (0, 0, 255),\n",
    "                             'fear': (128, 0, 128), 'surprise': (0, 255, 255), 'neutral': (220, 220, 220)}\n",
    "                color = color_map.get(emotion, (255, 255, 255))\n",
    "                \n",
    "                # Draw bounding box and emotion text\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{emotion.upper()} ({confidence:.2f})\", \n",
    "                            (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                \n",
    "                # Display scores for top 3 emotions\n",
    "                y_offset = 60\n",
    "                top_emotions = sorted(emotion_data['all_emotions'].items(), key=lambda i: i[1], reverse=True)\n",
    "                for i, (name, score) in enumerate(top_emotions[:3]):\n",
    "                    if name == 'disgust': continue # Skip disgust for a cleaner UI\n",
    "                    cv2.putText(frame, f\"{name.title()}: {score:.2f}\", (10, y_offset + i*25), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        # Display status indicator\n",
    "        status_text = \"RECORDING\" if interview_started else \"READY: Press 's' to start\"\n",
    "        status_color = (0, 0, 255) if interview_started else (0, 255, 0)\n",
    "        cv2.putText(frame, status_text, (10, frame.shape[0] - 15), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)\n",
    "        \n",
    "        cv2.imshow('AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_interview_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f8f937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 AI Interview Emotion Analysis (v3)\n",
      "Press 's' to START or RESTART the interview.\n",
      "Press 'q' to END interview and generate the report.\n",
      "🎥 Interview session started: 20250906_002246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(2, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Full data saved to: interview_20250906_002246_summary.json & _detailed.csv\n",
      "\n",
      "📊 Interview completed. Duration: 32.9s\n",
      "📄 Concise report saved to: final_report_summary_20250906_002246.json\n",
      "\n",
      "============================================================\n",
      "📊 FINAL INTERVIEW REPORT\n",
      "============================================================\n",
      "Overall Assessment: Excellent: Candidate was highly composed and emotionally stable.\n",
      "Emotional Stability: 94.0%\n",
      "Dominant Emotion Profile: Neutral\n",
      "\n",
      "🎭 Emotional Breakdown (% of time):\n",
      "  - Angry     : 28.09%\n",
      "  - Happy     : 10.9%\n",
      "  - Neutral   : 51.33%\n",
      "  - Sad       : 9.69%\n",
      "\n",
      "🚨 Critical Moments Summary:\n",
      "  - High Stress: 1 instance(s)\n",
      "  - Prolonged Neutrality: 1 instance(s)\n",
      "\n",
      "💡 Recommendations:\n",
      "  • Probe gently into topics that may have caused stress.\n",
      "  • Candidate may seem disengaged; try more dynamic questions.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class InterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes facial emotions in real-time during an interview, providing insights\n",
    "    into the candidate's emotional state, stability, and critical moments.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Use deques for efficient, fixed-size history tracking\n",
    "        self.emotion_history = deque(maxlen=15)  # Smoothed over last 15 frames\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'critical_moments': []\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        # Reset all tracking variables for a clean start\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Interview session started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Analyzes a single video frame for emotions, logs the data, and detects\n",
    "        critical moments. Returns processed data for display.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None  # Do not start automatically, wait for user input\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        # Primary face's emotion data\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        # Get a smoothed emotion to reduce rapid fluctuations\n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "\n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'raw_emotion': dominant_emotion,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "            'face_box': primary_face[\"box\"]\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        self.current_session['emotion_timeline'].append(emotion_entry)\n",
    "        \n",
    "        self.detect_critical_moments(emotion_entry)\n",
    "        \n",
    "        return emotion_entry\n",
    "\n",
    "    def detect_critical_moments(self, emotion_entry):\n",
    "        \"\"\"\n",
    "        Identifies and flags moments of significant emotional shifts, stress,\n",
    "        or disengagement based on refined heuristics.\n",
    "        \"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        all_emotions = emotion_entry['all_emotions']\n",
    "        \n",
    "        # --- Refined Critical Moment Detection ---\n",
    "\n",
    "        # 1. High Sadness or Fear (Strong negative indicators)\n",
    "        if (emotion == 'sad' and confidence > 0.35) or (emotion == 'fear' and confidence > 0.4):\n",
    "            self._log_critical_moment('high_stress', emotion_entry, \n",
    "                                      f\"Candidate showed strong signs of {emotion}.\")\n",
    "\n",
    "        # 2. Prolonged Neutrality (Potential Disengagement/Dullness)\n",
    "        if emotion == 'neutral':\n",
    "            self.neutral_streak += 1\n",
    "            if self.neutral_streak == 30: # Flag after ~30 consecutive neutral frames\n",
    "                self._log_critical_moment('prolonged_neutrality', emotion_entry,\n",
    "                                          \"Candidate showed a prolonged neutral expression (potential disinterest).\")\n",
    "        else:\n",
    "            self.neutral_streak = 0 # Reset streak if not neutral\n",
    "\n",
    "        # 3. Low Confidence / Uncertainty (Improved Logic)\n",
    "        is_ambiguous_negative = (\n",
    "            all_emotions.get('neutral', 0) > 0.20 and\n",
    "            (all_emotions.get('sad', 0) + all_emotions.get('fear', 0)) > 0.25 and\n",
    "            all_emotions.get('happy', 0) < 0.10\n",
    "        )\n",
    "\n",
    "        if confidence < 0.45 and is_ambiguous_negative:\n",
    "            self.low_confidence_streak += 1\n",
    "            if self.low_confidence_streak == 15: # Flag only after 15 frames of this state\n",
    "                self._log_critical_moment('low_confidence', emotion_entry, \n",
    "                                          \"Candidate appeared uncertain or less confident.\")\n",
    "        else:\n",
    "            self.low_confidence_streak = 0 # Reset streak\n",
    "\n",
    "    def _log_critical_moment(self, type, entry, description):\n",
    "        \"\"\"Helper to append a critical moment to the session log.\"\"\"\n",
    "        # Prevents logging the same type of moment repeatedly in a short time\n",
    "        if self.current_session['critical_moments']:\n",
    "            last_moment = self.current_session['critical_moments'][-1]\n",
    "            if last_moment['type'] == type and (entry['elapsed_seconds'] - last_moment['elapsed_seconds']) < 5:\n",
    "                return \n",
    "\n",
    "        self.current_session['critical_moments'].append({\n",
    "            'type': type,\n",
    "            'timestamp': entry['timestamp'],\n",
    "            'elapsed_seconds': entry['elapsed_seconds'],\n",
    "            'emotion': entry['dominant_emotion'],\n",
    "            'confidence': entry['confidence'],\n",
    "            'description': description\n",
    "        })\n",
    "        \n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates all reports.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return None\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_emotion_summary()\n",
    "        self.save_session_data()\n",
    "        \n",
    "        print(f\"\\n📊 Interview completed. Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "        return self.current_session\n",
    "\n",
    "    def generate_emotion_summary(self):\n",
    "        \"\"\"Calculates and stores aggregate emotion statistics for the session.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        emotion_changes = (df['dominant_emotion'].shift() != df['dominant_emotion']).sum()\n",
    "        stability_score = max(0, 100 - (emotion_changes / len(df) * 100)) if len(df) > 0 else 100\n",
    "        \n",
    "        critical_moment_counts = defaultdict(int)\n",
    "        for moment in self.current_session['critical_moments']:\n",
    "            critical_moment_counts[moment['type']] += 1\n",
    "\n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages, key=emotion_percentages.get, default='neutral'),\n",
    "            'critical_moment_counts': dict(critical_moment_counts)\n",
    "        }\n",
    "\n",
    "    def save_session_data(self):\n",
    "        \"\"\"Saves a detailed CSV and a summary JSON file for the session.\"\"\"\n",
    "        session_id = self.current_session['session_id']\n",
    "        with open(f'interview_{session_id}_summary.json', 'w') as f:\n",
    "            json.dump(self.current_session, f, indent=4, default=str)\n",
    "        if self.emotion_log:\n",
    "            pd.DataFrame(self.emotion_log).to_csv(f'interview_{session_id}_detailed.csv', index=False)\n",
    "        print(f\"💾 Full data saved to: interview_{session_id}_summary.json & _detailed.csv\")\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Generates a structured, human-readable report from the session data.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No summary available.\"}\n",
    "        \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary.get('critical_moment_counts', {}).values())\n",
    "\n",
    "        if stability >= 75 and stress_total <= 2:\n",
    "            assessment = \"Excellent: Candidate was highly composed and emotionally stable.\"\n",
    "        elif stability >= 55 and stress_total <= 5:\n",
    "            assessment = \"Good: Candidate showed solid emotional control with minor signs of stress.\"\n",
    "        elif stability >= 35 or summary.get('critical_moment_counts', {}).get('low_confidence', 0) > 0:\n",
    "            assessment = \"Fair: Candidate experienced some emotional fluctuations or uncertainty.\"\n",
    "        else:\n",
    "            assessment = \"Needs Attention: Candidate showed significant emotional instability or stress.\"\n",
    "\n",
    "        recommendations = []\n",
    "        counts = summary.get('critical_moment_counts', {})\n",
    "        if counts.get('high_stress', 0) > 0:\n",
    "            recommendations.append(\"Probe gently into topics that may have caused stress.\")\n",
    "        if counts.get('low_confidence', 0) > 0:\n",
    "            recommendations.append(\"Candidate showed uncertainty; consider asking questions to boost comfort.\")\n",
    "        if counts.get('prolonged_neutrality', 0) > 0:\n",
    "            recommendations.append(\"Candidate may seem disengaged; try more dynamic questions.\")\n",
    "        if not recommendations:\n",
    "            recommendations.append(\"Candidate performed well emotionally. No specific concerns noted.\")\n",
    "            \n",
    "        return {\n",
    "            'assessment': assessment,\n",
    "            'stability_score': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "            'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "            'emotion_breakdown': summary['emotion_percentages'],\n",
    "            'critical_moments_summary': dict(summary.get('critical_moment_counts', {})),\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "\n",
    "\n",
    "def run_interview_analysis():\n",
    "    \"\"\"Main loop to capture video, run analysis, and display results.\"\"\"\n",
    "    analyzer = InterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 AI Interview Emotion Analysis (v3)\")\n",
    "    print(\"Press 's' to START or RESTART the interview.\")\n",
    "    print(\"Press 'q' to END interview and generate the report.\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # *** NEW: Create and save the concise JSON report ***\n",
    "                if 'error' not in report:\n",
    "                    session_id = analyzer.current_session['session_id']\n",
    "                    # Select only the specific keys you requested\n",
    "                    concise_report_data = {\n",
    "                        \"Emotional Stability\": report['stability_score'],\n",
    "                        \"Dominant Emotion Profile\": report['dominant_emotion'],\n",
    "                        \"Emotional Breakdown (% of time)\": report['emotion_breakdown'],\n",
    "                        \"Critical Moments Summary\": report['critical_moments_summary']\n",
    "                    }\n",
    "                    report_filename = f\"final_report_summary_{session_id}.json\"\n",
    "                    with open(report_filename, 'w') as f:\n",
    "                        json.dump(concise_report_data, f, indent=4)\n",
    "                    print(f\"📄 Concise report saved to: {report_filename}\")\n",
    "                # *** END NEW SECTION ***\n",
    "\n",
    "                # Display final report in console\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"📊 FINAL INTERVIEW REPORT\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"Overall Assessment: {report.get('assessment', 'N/A')}\")\n",
    "                print(f\"Emotional Stability: {report.get('stability_score', 'N/A')}\")\n",
    "                print(f\"Dominant Emotion Profile: {report.get('dominant_emotion', 'N/A')}\")\n",
    "                \n",
    "                print(\"\\n🎭 Emotional Breakdown (% of time):\")\n",
    "                for emotion, pct in sorted(report.get('emotion_breakdown', {}).items()):\n",
    "                    print(f\"  - {emotion.title():<10}: {pct}%\")\n",
    "                \n",
    "                if report.get('critical_moments_summary'):\n",
    "                    print(\"\\n🚨 Critical Moments Summary:\")\n",
    "                    for type, count in report['critical_moments_summary'].items():\n",
    "                        print(f\"  - {type.replace('_', ' ').title()}: {count} instance(s)\")\n",
    "\n",
    "                print(\"\\n💡 Recommendations:\")\n",
    "                for rec in report.get('recommendations', []):\n",
    "                    print(f\"  • {rec}\")\n",
    "                print(\"=\"*60)\n",
    "            break\n",
    "        \n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            if emotion_data:\n",
    "                x, y, w, h = emotion_data['face_box']\n",
    "                emotion = emotion_data['dominant_emotion']\n",
    "                confidence = emotion_data['confidence']\n",
    "                \n",
    "                color_map = {'happy': (0, 255, 0), 'sad': (255, 100, 0), 'angry': (0, 0, 255),\n",
    "                             'fear': (128, 0, 128), 'surprise': (0, 255, 255), 'neutral': (220, 220, 220)}\n",
    "                color = color_map.get(emotion, (255, 255, 255))\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{emotion.upper()} ({confidence:.2f})\", \n",
    "                            (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                \n",
    "                y_offset = 60\n",
    "                top_emotions = sorted(emotion_data['all_emotions'].items(), key=lambda i: i[1], reverse=True)\n",
    "                for i, (name, score) in enumerate(top_emotions[:3]):\n",
    "                    if name == 'disgust': continue\n",
    "                    cv2.putText(frame, f\"{name.title()}: {score:.2f}\", (10, y_offset + i*25), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        status_text = \"RECORDING\" if interview_started else \"READY: Press 's' to start\"\n",
    "        status_color = (0, 0, 255) if interview_started else (0, 255, 0)\n",
    "        cv2.putText(frame, status_text, (10, frame.shape[0] - 15), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)\n",
    "        \n",
    "        cv2.imshow('AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_interview_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4779a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\fer\\fer.py:34: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 AI Interview Emotion Analysis\n",
      "Press 's' to START interview.\n",
      "Press 'q' to END interview and save the report.\n",
      "🎥 Interview session started: 20250906_001914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(2, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(3, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Interview completed. Duration: 35.1s\n",
      "📄 Report saved to: final_report_summary_20250906_001914.json\n",
      "\n",
      "============================================================\n",
      "📊 FINAL INTERVIEW REPORT\n",
      "============================================================\n",
      "Assessment: Excellent\n",
      "Stability Score: 96.7%\n",
      "Dominant Emotion: Neutral\n",
      "Emotion Breakdown Percentage:\n",
      "  - Neutral: 76.67\n",
      "  - Happy: 20.67\n",
      "  - Angry: 2.67\n",
      "Critical Moments Summary:\n",
      "  - Prolonged_Neutrality: 2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class InterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes facial emotions in real-time during an interview, providing insights\n",
    "    into the candidate's emotional state, stability, and critical moments.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Use deques for efficient, fixed-size history tracking\n",
    "        self.emotion_history = deque(maxlen=15)  # Smoothed over last 15 frames\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'critical_moments': []\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        # Reset all tracking variables for a clean start\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Interview session started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Analyzes a single video frame for emotions, logs the data, and detects\n",
    "        critical moments. Returns processed data for display.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "\n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "            'face_box': primary_face[\"box\"]\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        self.detect_critical_moments(emotion_entry)\n",
    "        return emotion_entry\n",
    "\n",
    "    def detect_critical_moments(self, emotion_entry):\n",
    "        \"\"\"\n",
    "        Identifies and flags moments of significant emotional shifts, stress,\n",
    "        or disengagement based on refined heuristics.\n",
    "        \"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        all_emotions = emotion_entry['all_emotions']\n",
    "\n",
    "        if (emotion == 'sad' and confidence > 0.35) or (emotion == 'fear' and confidence > 0.4):\n",
    "            self._log_critical_moment('high_stress', emotion_entry, f\"Candidate showed strong signs of {emotion}.\")\n",
    "\n",
    "        if emotion == 'neutral':\n",
    "            self.neutral_streak += 1\n",
    "            if self.neutral_streak == 30:\n",
    "                self._log_critical_moment('prolonged_neutrality', emotion_entry, \"Candidate showed a prolonged neutral expression.\")\n",
    "        else:\n",
    "            self.neutral_streak = 0\n",
    "\n",
    "        is_ambiguous = (all_emotions.get('neutral', 0) > 0.2 and (all_emotions.get('sad', 0) + all_emotions.get('fear', 0)) > 0.25)\n",
    "        if confidence < 0.45 and is_ambiguous:\n",
    "            self.low_confidence_streak += 1\n",
    "            if self.low_confidence_streak == 15:\n",
    "                self._log_critical_moment('low_confidence', emotion_entry, \"Candidate appeared uncertain.\")\n",
    "        else:\n",
    "            self.low_confidence_streak = 0\n",
    "\n",
    "    def _log_critical_moment(self, type, entry, description):\n",
    "        \"\"\"Helper to append a critical moment to the session log, avoiding rapid duplicates.\"\"\"\n",
    "        if self.current_session['critical_moments']:\n",
    "            last_moment = self.current_session['critical_moments'][-1]\n",
    "            if last_moment['type'] == type and (entry['elapsed_seconds'] - last_moment['elapsed_seconds']) < 5:\n",
    "                return\n",
    "        self.current_session['critical_moments'].append({\n",
    "            'type': type, 'timestamp': entry['timestamp'], 'elapsed_seconds': entry['elapsed_seconds'],\n",
    "            'emotion': entry['dominant_emotion'], 'confidence': entry['confidence'], 'description': description\n",
    "        })\n",
    "        \n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates the summary.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_emotion_summary()\n",
    "        \n",
    "        print(f\"\\n📊 Interview completed. Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "\n",
    "    def generate_emotion_summary(self):\n",
    "        \"\"\"Calculates and stores aggregate emotion statistics for the session.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        emotion_changes = (df['dominant_emotion'].shift() != df['dominant_emotion']).sum()\n",
    "        stability_score = max(0, 100 - (emotion_changes / len(df) * 100)) if len(df) > 0 else 100\n",
    "        \n",
    "        critical_moment_counts = defaultdict(int)\n",
    "        for moment in self.current_session['critical_moments']:\n",
    "            critical_moment_counts[moment['type']] += 1\n",
    "\n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages, key=emotion_percentages.get, default='neutral'),\n",
    "            'critical_moment_counts': dict(critical_moment_counts)\n",
    "        }\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Generates a structured, human-readable report from the session data.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No summary available.\"}\n",
    "        \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary.get('critical_moment_counts', {}).values())\n",
    "\n",
    "        if stability >= 75 and stress_total <= 2: assessment = \"Excellent\"\n",
    "        elif stability >= 55 and stress_total <= 5: assessment = \"Good\"\n",
    "        elif stability >= 35 or summary.get('critical_moment_counts', {}).get('low_confidence', 0) > 0: assessment = \"Fair\"\n",
    "        else: assessment = \"Needs Attention\"\n",
    "            \n",
    "        return {\n",
    "            'assessment': assessment,\n",
    "            'stability_score': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "            'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "            'emotion_breakdown_percentage': summary['emotion_percentages'],\n",
    "            'critical_moments_summary': dict(summary.get('critical_moment_counts', {}))\n",
    "        }\n",
    "\n",
    "\n",
    "def run_interview_analysis():\n",
    "    \"\"\"Main loop to capture video, run analysis, and display results.\"\"\"\n",
    "    analyzer = InterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 AI Interview Emotion Analysis\")\n",
    "    print(\"Press 's' to START interview.\")\n",
    "    print(\"Press 'q' to END interview and save the report.\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # Create and save the single, concise JSON report\n",
    "                if 'error' not in report:\n",
    "                    session_id = analyzer.current_session['session_id']\n",
    "                    report_filename = f\"final_report_summary_{session_id}.json\"\n",
    "                    with open(report_filename, 'w') as f:\n",
    "                        json.dump(report, f, indent=4)\n",
    "                    print(f\"📄 Report saved to: {report_filename}\")\n",
    "\n",
    "                # Display final report in console\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"📊 FINAL INTERVIEW REPORT\")\n",
    "                print(\"=\"*60)\n",
    "                for key, value in report.items():\n",
    "                    if isinstance(value, dict):\n",
    "                        print(f\"{key.replace('_', ' ').title()}:\")\n",
    "                        for sub_key, sub_value in value.items():\n",
    "                            print(f\"  - {sub_key.title()}: {sub_value}\")\n",
    "                    else:\n",
    "                        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "                print(\"=\"*60)\n",
    "            break\n",
    "        \n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            if emotion_data:\n",
    "                x, y, w, h = emotion_data['face_box']\n",
    "                emotion = emotion_data['dominant_emotion']\n",
    "                confidence = emotion_data['confidence']\n",
    "                \n",
    "                color_map = {'happy': (0, 255, 0), 'sad': (255, 100, 0), 'angry': (0, 0, 255),\n",
    "                             'fear': (128, 0, 128), 'surprise': (0, 255, 255), 'neutral': (220, 220, 220)}\n",
    "                color = color_map.get(emotion, (255, 255, 255))\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{emotion.upper()} ({confidence:.2f})\", \n",
    "                            (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        status_text = \"RECORDING\" if interview_started else \"READY: Press 's' to start\"\n",
    "        status_color = (0, 0, 255) if interview_started else (0, 255, 0)\n",
    "        cv2.putText(frame, status_text, (10, frame.shape[0] - 15), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)\n",
    "        \n",
    "        cv2.imshow('AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_interview_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72a8fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 AI Interview Emotion Analysis\n",
      "Press 's' to START interview.\n",
      "Press 'q' to END interview and save the report.\n",
      "🎥 Interview session started: 20250906_002359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(2, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(3, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Interview completed. Duration: 59.0s\n",
      "📄 Report saved to: final_report_summary_20250906_002359.json\n",
      "\n",
      "============================================================\n",
      "📊 FINAL INTERVIEW REPORT\n",
      "============================================================\n",
      "Assessment: Good\n",
      "Stability Score: 95.5%\n",
      "Dominant Emotion: Neutral\n",
      "Emotion Breakdown Percentage:\n",
      "  - Neutral: 66.82\n",
      "  - Happy: 26.11\n",
      "  - Angry: 4.45\n",
      "  - Fear: 2.61\n",
      "Critical Moments Summary:\n",
      "  - Prolonged_Neutrality: 3\n",
      "  - High_Stress: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class InterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes facial emotions in real-time during an interview, providing insights\n",
    "    into the candidate's emotional state, stability, and critical moments.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Use deques for efficient, fixed-size history tracking\n",
    "        self.emotion_history = deque(maxlen=15)  # Smoothed over last 15 frames\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'critical_moments': []\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        # Reset all tracking variables for a clean start\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Interview session started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Analyzes a single video frame for emotions, logs the data, and detects\n",
    "        critical moments. Returns processed data for display.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "\n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "            'face_box': primary_face[\"box\"]\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        self.detect_critical_moments(emotion_entry)\n",
    "        return emotion_entry\n",
    "\n",
    "    def detect_critical_moments(self, emotion_entry):\n",
    "        \"\"\"\n",
    "        Identifies and flags moments of significant emotional shifts, stress,\n",
    "        or disengagement based on refined heuristics.\n",
    "        \"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        all_emotions = emotion_entry['all_emotions']\n",
    "\n",
    "        if (emotion == 'sad' and confidence > 0.35) or (emotion == 'fear' and confidence > 0.4):\n",
    "            self._log_critical_moment('high_stress', emotion_entry, f\"Candidate showed strong signs of {emotion}.\")\n",
    "\n",
    "        if emotion == 'neutral':\n",
    "            self.neutral_streak += 1\n",
    "            if self.neutral_streak == 30:\n",
    "                self._log_critical_moment('prolonged_neutrality', emotion_entry, \"Candidate showed a prolonged neutral expression.\")\n",
    "        else:\n",
    "            self.neutral_streak = 0\n",
    "\n",
    "        is_ambiguous = (all_emotions.get('neutral', 0) > 0.2 and (all_emotions.get('sad', 0) + all_emotions.get('fear', 0)) > 0.25)\n",
    "        if confidence < 0.45 and is_ambiguous:\n",
    "            self.low_confidence_streak += 1\n",
    "            if self.low_confidence_streak == 15:\n",
    "                self._log_critical_moment('low_confidence', emotion_entry, \"Candidate appeared uncertain.\")\n",
    "        else:\n",
    "            self.low_confidence_streak = 0\n",
    "\n",
    "    def _log_critical_moment(self, type, entry, description):\n",
    "        \"\"\"Helper to append a critical moment to the session log, avoiding rapid duplicates.\"\"\"\n",
    "        if self.current_session['critical_moments']:\n",
    "            last_moment = self.current_session['critical_moments'][-1]\n",
    "            if last_moment['type'] == type and (entry['elapsed_seconds'] - last_moment['elapsed_seconds']) < 5:\n",
    "                return\n",
    "        self.current_session['critical_moments'].append({\n",
    "            'type': type, 'timestamp': entry['timestamp'], 'elapsed_seconds': entry['elapsed_seconds'],\n",
    "            'emotion': entry['dominant_emotion'], 'confidence': entry['confidence'], 'description': description\n",
    "        })\n",
    "        \n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates the summary.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_emotion_summary()\n",
    "        \n",
    "        print(f\"\\n📊 Interview completed. Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "\n",
    "    def generate_emotion_summary(self):\n",
    "        \"\"\"Calculates and stores aggregate emotion statistics for the session.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        emotion_changes = (df['dominant_emotion'].shift() != df['dominant_emotion']).sum()\n",
    "        stability_score = max(0, 100 - (emotion_changes / len(df) * 100)) if len(df) > 0 else 100\n",
    "        \n",
    "        critical_moment_counts = defaultdict(int)\n",
    "        for moment in self.current_session['critical_moments']:\n",
    "            critical_moment_counts[moment['type']] += 1\n",
    "\n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages, key=emotion_percentages.get, default='neutral'),\n",
    "            'critical_moment_counts': dict(critical_moment_counts)\n",
    "        }\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Generates a structured, human-readable report from the session data.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No summary available.\"}\n",
    "        \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary.get('critical_moment_counts', {}).values())\n",
    "\n",
    "        if stability >= 75 and stress_total <= 2: assessment = \"Excellent\"\n",
    "        elif stability >= 55 and stress_total <= 5: assessment = \"Good\"\n",
    "        elif stability >= 35 or summary.get('critical_moment_counts', {}).get('low_confidence', 0) > 0: assessment = \"Fair\"\n",
    "        else: assessment = \"Needs Attention\"\n",
    "            \n",
    "        return {\n",
    "            'assessment': assessment,\n",
    "            'stability_score': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "            'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "            'emotion_breakdown_percentage': summary['emotion_percentages'],\n",
    "            'critical_moments_summary': dict(summary.get('critical_moment_counts', {}))\n",
    "        }\n",
    "\n",
    "\n",
    "def run_interview_analysis():\n",
    "    \"\"\"Main loop to capture video, run analysis, and display results.\"\"\"\n",
    "    analyzer = InterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 AI Interview Emotion Analysis\")\n",
    "    print(\"Press 's' to START interview.\")\n",
    "    print(\"Press 'q' to END interview and save the report.\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # Create and save the single, concise JSON report\n",
    "                if 'error' not in report:\n",
    "                    session_id = analyzer.current_session['session_id']\n",
    "                    report_filename = f\"final_report_summary_{session_id}.json\"\n",
    "                    with open(report_filename, 'w') as f:\n",
    "                        json.dump(report, f, indent=4)\n",
    "                    print(f\"📄 Report saved to: {report_filename}\")\n",
    "\n",
    "                # Display final report in console\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"📊 FINAL INTERVIEW REPORT\")\n",
    "                print(\"=\"*60)\n",
    "                for key, value in report.items():\n",
    "                    if isinstance(value, dict):\n",
    "                        print(f\"{key.replace('_', ' ').title()}:\")\n",
    "                        for sub_key, sub_value in value.items():\n",
    "                            print(f\"  - {sub_key.title()}: {sub_value}\")\n",
    "                    else:\n",
    "                        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "                print(\"=\"*60)\n",
    "            break\n",
    "        \n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            if emotion_data:\n",
    "                x, y, w, h = emotion_data['face_box']\n",
    "                emotion = emotion_data['dominant_emotion']\n",
    "                confidence = emotion_data['confidence']\n",
    "                \n",
    "                color_map = {'happy': (0, 255, 0), 'sad': (255, 100, 0), 'angry': (0, 0, 255),\n",
    "                             'fear': (128, 0, 128), 'surprise': (0, 255, 255), 'neutral': (220, 220, 220)}\n",
    "                color = color_map.get(emotion, (255, 255, 255))\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{emotion.upper()} ({confidence:.2f})\", \n",
    "                            (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        status_text = \"RECORDING\" if interview_started else \"READY: Press 's' to start\"\n",
    "        status_color = (0, 0, 255) if interview_started else (0, 255, 0)\n",
    "        cv2.putText(frame, status_text, (10, frame.shape[0] - 15), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)\n",
    "        \n",
    "        cv2.imshow('AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_interview_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe5ffd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 COMPLETELY FIXED AI Interview Emotion Analysis\n",
      "Features: Gaze Tracking, Sustained Confidence Detection\n",
      "Excluded: Anger and Sadness emotions\n",
      "\n",
      "CORRECTED LOGIC:\n",
      "✅ Looking at camera = HIGH confidence (emotion > 0.15) or MEDIUM (always)\n",
      "❌ Looking left/right = LOW confidence\n",
      "🔄 Looking up/down = MEDIUM/LOW confidence\n",
      "\n",
      "Controls:\n",
      "Press 's' to START interview\n",
      "Press 'q' to END interview and generate report\n",
      "🎥 Enhanced Interview Analysis Started: 20250906_010423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(2, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Enhanced Interview Analysis Complete!\n",
      "Duration: 21.5s\n",
      "📄 Report saved: completely_fixed_interview_report_20250906_010423.json\n",
      "\n",
      "======================================================================\n",
      "📊 COMPLETELY FIXED INTERVIEW ANALYSIS REPORT\n",
      "======================================================================\n",
      "Session Duration: 21.5 seconds\n",
      "Dominant Emotion: Neutral\n",
      "Engagement Score: 23.1%\n",
      "Overall Confidence: Good\n",
      "Camera Focus Time: 23.1%\n",
      "Low Confidence Duration: 0.0s\n",
      "\n",
      "📈 Detailed Metrics:\n",
      "Gaze Distribution: {'up': 76.92, 'center': 23.08}\n",
      "Confidence Breakdown: {'medium': 76.92, 'high': 23.08}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "\n",
    "class ImprovedInterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Enhanced emotion analyzer that tracks gaze direction and excludes anger/sadness.\n",
    "    Only reports low confidence when sustained for more than 1 second.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Enhanced tracking variables\n",
    "        self.emotion_history = deque(maxlen=15)\n",
    "        self.low_confidence_start_time = None\n",
    "        self.sustained_low_confidence_count = 0\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'gaze_analysis': {},\n",
    "            'confidence_metrics': {}\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.low_confidence_start_time = None\n",
    "        self.sustained_low_confidence_count = 0\n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Enhanced Interview Analysis Started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def detect_gaze_direction(self, frame, face_box):\n",
    "        \"\"\"\n",
    "        Detects gaze direction based on face and eye positions.\n",
    "        Returns: 'center', 'left', 'right', 'up', 'down'\n",
    "        \"\"\"\n",
    "        x, y, w, h = face_box\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect eyes within the face region\n",
    "        eyes = self.eye_cascade.detectMultiScale(face_gray, 1.1, 5)\n",
    "        \n",
    "        if len(eyes) < 2:\n",
    "            return 'center'  # Default to center if can't detect both eyes\n",
    "        \n",
    "        # Calculate eye positions relative to face center\n",
    "        face_center_x = w // 2\n",
    "        face_center_y = h // 2\n",
    "        \n",
    "        # Get the two largest eye detections (most likely to be actual eyes)\n",
    "        eyes_sorted = sorted(eyes, key=lambda e: e[2] * e[3], reverse=True)[:2]\n",
    "        \n",
    "        eye_centers = []\n",
    "        for (ex, ey, ew, eh) in eyes_sorted:\n",
    "            eye_center_x = ex + ew // 2\n",
    "            eye_center_y = ey + eh // 2\n",
    "            eye_centers.append((eye_center_x, eye_center_y))\n",
    "        \n",
    "        if len(eye_centers) == 2:\n",
    "            # Average eye position\n",
    "            avg_eye_x = (eye_centers[0][0] + eye_centers[1][0]) // 2\n",
    "            avg_eye_y = (eye_centers[0][1] + eye_centers[1][1]) // 2\n",
    "            \n",
    "            # Calculate relative position to face center\n",
    "            horizontal_deviation = avg_eye_x - face_center_x\n",
    "            vertical_deviation = avg_eye_y - face_center_y\n",
    "            \n",
    "            # Thresholds (percentage of face dimensions)\n",
    "            horizontal_threshold = w * 0.15  # Increased for better center detection\n",
    "            vertical_threshold = h * 0.12    # Increased for better center detection\n",
    "            \n",
    "            # Determine gaze direction\n",
    "            if abs(horizontal_deviation) <= horizontal_threshold and abs(vertical_deviation) <= vertical_threshold:\n",
    "                return 'center'\n",
    "            elif horizontal_deviation < -horizontal_threshold:\n",
    "                return 'left'\n",
    "            elif horizontal_deviation > horizontal_threshold:\n",
    "                return 'right'\n",
    "            elif vertical_deviation < -vertical_threshold:\n",
    "                return 'up'\n",
    "            elif vertical_deviation > vertical_threshold:\n",
    "                return 'down'\n",
    "        \n",
    "        return 'center'  # Default to center if unclear\n",
    "\n",
    "    def calculate_confidence_level(self, gaze_direction, emotion_scores):\n",
    "        \"\"\"\n",
    "        COMPLETELY FIXED: Determines confidence level based on gaze direction and emotion clarity.\n",
    "        Returns: 'high', 'medium', 'low'\n",
    "        \"\"\"\n",
    "        # Filter out anger and sadness from consideration\n",
    "        filtered_emotions = {k: v for k, v in emotion_scores.items() \n",
    "                           if k not in ['angry', 'sad']}\n",
    "        \n",
    "        if not filtered_emotions:\n",
    "            # If no emotions detected, use gaze as primary indicator\n",
    "            if gaze_direction == 'center':\n",
    "                return 'medium'  # At least medium when looking at camera\n",
    "            else:\n",
    "                return 'low'\n",
    "        \n",
    "        max_emotion_confidence = max(filtered_emotions.values())\n",
    "        \n",
    "        # NEW FIXED LOGIC - GAZE IS PRIMARY, EMOTION IS SECONDARY:\n",
    "        \n",
    "        # 1. CENTER gaze = HIGH or MEDIUM confidence (NEVER LOW)\n",
    "        if gaze_direction == 'center':\n",
    "            if max_emotion_confidence > 0.15:  # Very low threshold\n",
    "                return 'high'\n",
    "            else:\n",
    "                return 'medium'  # ALWAYS at least medium when looking at camera\n",
    "        \n",
    "        # 2. LEFT or RIGHT gaze = ALWAYS LOW confidence\n",
    "        elif gaze_direction in ['left', 'right']:\n",
    "            return 'low'\n",
    "        \n",
    "        # 3. UP, DOWN, or UNCERTAIN gaze = depends on emotion\n",
    "        elif gaze_direction in ['up', 'down', 'uncertain']:\n",
    "            if max_emotion_confidence > 0.4:\n",
    "                return 'medium'\n",
    "            else:\n",
    "                return 'low'\n",
    "        \n",
    "        # 4. Fallback\n",
    "        else:\n",
    "            return 'low'\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Enhanced frame analysis with gaze tracking and sustained low confidence detection.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        face_box = primary_face[\"box\"]\n",
    "        \n",
    "        # Remove anger and sadness from analysis\n",
    "        filtered_emotions = {k: v for k, v in emotion_scores.items() \n",
    "                           if k not in ['angry', 'sad']}\n",
    "        \n",
    "        if filtered_emotions:\n",
    "            dominant_emotion = max(filtered_emotions, key=filtered_emotions.get)\n",
    "            dominant_confidence = filtered_emotions[dominant_emotion]\n",
    "        else:\n",
    "            dominant_emotion = 'neutral'\n",
    "            dominant_confidence = 0.5\n",
    "        \n",
    "        # Detect gaze direction\n",
    "        gaze_direction = self.detect_gaze_direction(frame, face_box)\n",
    "        \n",
    "        # Calculate confidence level (COMPLETELY FIXED)\n",
    "        confidence_level = self.calculate_confidence_level(gaze_direction, filtered_emotions)\n",
    "        \n",
    "        # Track sustained low confidence (only count if sustained for >1 second)\n",
    "        is_sustained_low_confidence = False\n",
    "        if confidence_level == 'low':\n",
    "            if self.low_confidence_start_time is None:\n",
    "                self.low_confidence_start_time = current_time\n",
    "            elif (current_time - self.low_confidence_start_time).total_seconds() > 1.0:\n",
    "                is_sustained_low_confidence = True\n",
    "                self.sustained_low_confidence_count += 1\n",
    "        else:\n",
    "            self.low_confidence_start_time = None\n",
    "        \n",
    "        # Add to emotion history for smoothing\n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), \n",
    "                             key=list(self.emotion_history).count)\n",
    "        \n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'confidence': round(dominant_confidence, 3),\n",
    "            'gaze_direction': gaze_direction,\n",
    "            'confidence_level': confidence_level,\n",
    "            'sustained_low_confidence': is_sustained_low_confidence,\n",
    "            'filtered_emotions': {k: round(v, 3) for k, v in filtered_emotions.items()},\n",
    "            'face_box': face_box\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        return emotion_entry\n",
    "\n",
    "    # ... (rest of your existing methods remain the same) ...\n",
    "\n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates comprehensive summary.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return None\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_comprehensive_summary()\n",
    "        \n",
    "        print(f\"\\n📊 Enhanced Interview Analysis Complete!\")\n",
    "        print(f\"Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "        return self.current_session\n",
    "\n",
    "    def generate_comprehensive_summary(self):\n",
    "        \"\"\"Generates detailed analysis including gaze and confidence metrics.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        \n",
    "        # Emotion analysis (excluding anger and sadness)\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        \n",
    "        # Gaze analysis\n",
    "        gaze_percentages = (df['gaze_direction'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        \n",
    "        # Confidence analysis\n",
    "        confidence_percentages = (df['confidence_level'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        \n",
    "        # Calculate engagement score (higher when looking at camera)\n",
    "        engagement_score = gaze_percentages.get('center', 0)\n",
    "        \n",
    "        # Calculate sustained low confidence duration\n",
    "        total_frames = len(df)\n",
    "        low_confidence_frames = df['sustained_low_confidence'].sum()\n",
    "        fps_estimate = 30  # Assuming 30 FPS\n",
    "        low_confidence_duration = round(low_confidence_frames / fps_estimate, 2)\n",
    "        \n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'dominant_emotion_overall': max(emotion_percentages.keys(), key=lambda k: emotion_percentages[k]) if emotion_percentages else 'neutral',\n",
    "        }\n",
    "        \n",
    "        self.current_session['gaze_analysis'] = {\n",
    "            'gaze_percentages': gaze_percentages,\n",
    "            'engagement_score': round(engagement_score, 1),\n",
    "            'looking_at_camera_percentage': gaze_percentages.get('center', 0)\n",
    "        }\n",
    "        \n",
    "        self.current_session['confidence_metrics'] = {\n",
    "            'confidence_distribution': confidence_percentages,\n",
    "            'high_confidence_percentage': confidence_percentages.get('high', 0),\n",
    "            'sustained_low_confidence_duration_seconds': low_confidence_duration,\n",
    "            'overall_confidence_rating': self._calculate_overall_confidence_rating(confidence_percentages)\n",
    "        }\n",
    "\n",
    "    def _calculate_overall_confidence_rating(self, confidence_dist):\n",
    "        \"\"\"Calculate overall confidence rating from distribution.\"\"\"\n",
    "        high_pct = confidence_dist.get('high', 0)\n",
    "        medium_pct = confidence_dist.get('medium', 0)\n",
    "        low_pct = confidence_dist.get('low', 0)\n",
    "        \n",
    "        if high_pct >= 60:\n",
    "            return 'Excellent'\n",
    "        elif high_pct >= 40 or (high_pct + medium_pct) >= 70:\n",
    "            return 'Good'\n",
    "        elif (high_pct + medium_pct) >= 50:\n",
    "            return 'Fair'\n",
    "        else:\n",
    "            return 'Needs Improvement'\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Generates a comprehensive, readable final report.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No analysis data available.\"}\n",
    "        \n",
    "        return {\n",
    "            'session_info': {\n",
    "                'session_id': self.current_session['session_id'],\n",
    "                'duration': f\"{self.current_session['total_duration']:.1f} seconds\"\n",
    "            },\n",
    "            'emotional_state': {\n",
    "                'dominant_emotion': self.current_session['emotion_summary']['dominant_emotion_overall'].title(),\n",
    "                'emotion_breakdown': self.current_session['emotion_summary']['emotion_percentages']\n",
    "            },\n",
    "            'engagement_analysis': {\n",
    "                'overall_engagement_score': f\"{self.current_session['gaze_analysis']['engagement_score']:.1f}%\",\n",
    "                'looking_at_camera': f\"{self.current_session['gaze_analysis']['looking_at_camera_percentage']:.1f}%\",\n",
    "                'gaze_distribution': self.current_session['gaze_analysis']['gaze_percentages']\n",
    "            },\n",
    "            'confidence_assessment': {\n",
    "                'overall_rating': self.current_session['confidence_metrics']['overall_confidence_rating'],\n",
    "                'high_confidence_time': f\"{self.current_session['confidence_metrics']['high_confidence_percentage']:.1f}%\",\n",
    "                'low_confidence_duration': f\"{self.current_session['confidence_metrics']['sustained_low_confidence_duration_seconds']:.1f}s\",\n",
    "                'confidence_breakdown': self.current_session['confidence_metrics']['confidence_distribution']\n",
    "            }\n",
    "        }\n",
    "\n",
    "def run_enhanced_interview_analysis():\n",
    "    \"\"\"Main function to run the enhanced interview analysis.\"\"\"\n",
    "    analyzer = ImprovedInterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 COMPLETELY FIXED AI Interview Emotion Analysis\")\n",
    "    print(\"Features: Gaze Tracking, Sustained Confidence Detection\")\n",
    "    print(\"Excluded: Anger and Sadness emotions\")\n",
    "    print(\"\\nCORRECTED LOGIC:\")\n",
    "    print(\"✅ Looking at camera = HIGH confidence (emotion > 0.15) or MEDIUM (always)\")\n",
    "    print(\"❌ Looking left/right = LOW confidence\")\n",
    "    print(\"🔄 Looking up/down = MEDIUM/LOW confidence\")\n",
    "    print(\"\\nControls:\")\n",
    "    print(\"Press 's' to START interview\")\n",
    "    print(\"Press 'q' to END interview and generate report\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # Save detailed report\n",
    "                session_id = analyzer.current_session['session_id']\n",
    "                report_filename = f\"completely_fixed_interview_report_{session_id}.json\"\n",
    "                with open(report_filename, 'w') as f:\n",
    "                    json.dump(report, f, indent=4)\n",
    "                print(f\"📄 Report saved: {report_filename}\")\n",
    "\n",
    "                # Display summary\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"📊 COMPLETELY FIXED INTERVIEW ANALYSIS REPORT\")\n",
    "                print(\"=\"*70)\n",
    "                \n",
    "                print(f\"Session Duration: {report['session_info']['duration']}\")\n",
    "                print(f\"Dominant Emotion: {report['emotional_state']['dominant_emotion']}\")\n",
    "                print(f\"Engagement Score: {report['engagement_analysis']['overall_engagement_score']}\")\n",
    "                print(f\"Overall Confidence: {report['confidence_assessment']['overall_rating']}\")\n",
    "                print(f\"Camera Focus Time: {report['engagement_analysis']['looking_at_camera']}\")\n",
    "                print(f\"Low Confidence Duration: {report['confidence_assessment']['low_confidence_duration']}\")\n",
    "                \n",
    "                print(\"\\n📈 Detailed Metrics:\")\n",
    "                print(\"Gaze Distribution:\", report['engagement_analysis']['gaze_distribution'])\n",
    "                print(\"Confidence Breakdown:\", report['confidence_assessment']['confidence_breakdown'])\n",
    "                print(\"=\"*70)\n",
    "            break\n",
    "        \n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            if emotion_data:\n",
    "                x, y, w, h = emotion_data['face_box']\n",
    "                emotion = emotion_data['dominant_emotion']\n",
    "                confidence = emotion_data['confidence']\n",
    "                gaze = emotion_data['gaze_direction']\n",
    "                conf_level = emotion_data['confidence_level']\n",
    "                \n",
    "                # Color coding for confidence levels\n",
    "                color_map = {\n",
    "                    'high': (0, 255, 0),      # Green\n",
    "                    'medium': (0, 255, 255),  # Yellow\n",
    "                    'low': (0, 0, 255)        # Red\n",
    "                }\n",
    "                color = color_map.get(conf_level, (255, 255, 255))\n",
    "                \n",
    "                # Draw face rectangle\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                \n",
    "                # Display emotion and confidence info\n",
    "                cv2.putText(frame, f\"{emotion.upper()}\", \n",
    "                           (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                cv2.putText(frame, f\"Confidence: {conf_level.upper()}\", \n",
    "                           (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "                cv2.putText(frame, f\"Gaze: {gaze.upper()}\", \n",
    "                           (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Status display\n",
    "        status_text = \"🔴 RECORDING\" if interview_started else \"⚪ READY - Press 's' to start\"\n",
    "        status_color = (0, 0, 255) if interview_started else (0, 255, 0)\n",
    "        cv2.putText(frame, status_text, (10, frame.shape[0] - 20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)\n",
    "        \n",
    "        cv2.imshow('COMPLETELY FIXED AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_enhanced_interview_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efbbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 SIMPLE AI Interview Analysis\n",
      "SIMPLE RULES:\n",
      "✅ Looking at camera (CENTER) = HIGH confidence\n",
      "❌ Looking left/right = LOW confidence\n",
      "📊 Emotions detected: NEUTRAL, HAPPY only\n",
      "\n",
      "Controls:\n",
      "Press 's' to START interview\n",
      "Press 'q' to END interview\n",
      "🎥 Simple Interview Analysis Started: 20250906_014333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Simple Interview Analysis Complete!\n",
      "Duration: 52.9s\n",
      "📄 Report saved: simple_interview_report_20250906_014333.json\n",
      "\n",
      "==================================================\n",
      "📊 SIMPLE INTERVIEW REPORT\n",
      "==================================================\n",
      "Duration: 52.9 seconds\n",
      "Dominant Emotion: Neutral\n",
      "Looking at Camera: 97.0%\n",
      "High Confidence: 97.0%\n",
      "Low Confidence Duration: 0.0s\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "\n",
    "class SimpleInterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Simple emotion analyzer: \n",
    "    - Detects neutral, happy emotions (excludes anger, sadness)\n",
    "    - Center gaze = HIGH confidence\n",
    "    - Left/Right gaze = LOW confidence\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Simple tracking\n",
    "        self.emotion_history = deque(maxlen=10)\n",
    "        self.low_confidence_start_time = None\n",
    "        self.sustained_low_confidence_count = 0\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'gaze_analysis': {},\n",
    "            'confidence_metrics': {}\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.low_confidence_start_time = None\n",
    "        self.sustained_low_confidence_count = 0\n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Simple Interview Analysis Started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def detect_gaze_direction(self, frame, face_box):\n",
    "        \"\"\"\n",
    "        Simple gaze detection: center, left, right\n",
    "        \"\"\"\n",
    "        x, y, w, h = face_box\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        eyes = self.eye_cascade.detectMultiScale(face_gray, 1.1, 5)\n",
    "        \n",
    "        if len(eyes) < 2:\n",
    "            return 'center'  # Default to center\n",
    "        \n",
    "        face_center_x = w // 2\n",
    "        eyes_sorted = sorted(eyes, key=lambda e: e[2] * e[3], reverse=True)[:2]\n",
    "        \n",
    "        eye_centers_x = []\n",
    "        for (ex, ey, ew, eh) in eyes_sorted:\n",
    "            eye_center_x = ex + ew // 2\n",
    "            eye_centers_x.append(eye_center_x)\n",
    "        \n",
    "        if len(eye_centers_x) == 2:\n",
    "            avg_eye_x = sum(eye_centers_x) // 2\n",
    "            horizontal_deviation = avg_eye_x - face_center_x\n",
    "            threshold = w * 0.15  # 15% of face width\n",
    "            \n",
    "            if abs(horizontal_deviation) <= threshold:\n",
    "                return 'center'\n",
    "            elif horizontal_deviation < -threshold:\n",
    "                return 'left'\n",
    "            else:\n",
    "                return 'right'\n",
    "        \n",
    "        return 'center'\n",
    "\n",
    "    def calculate_confidence_level(self, gaze_direction):\n",
    "        \"\"\"\n",
    "        SIMPLE confidence logic:\n",
    "        - Center = HIGH confidence  \n",
    "        - Left/Right = LOW confidence\n",
    "        \"\"\"\n",
    "        if gaze_direction == 'center':\n",
    "            return 'high'\n",
    "        elif gaze_direction in ['left', 'right']:\n",
    "            return 'low'\n",
    "        else:\n",
    "            return 'high'  # Default\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"Simple frame analysis.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        face_box = primary_face[\"box\"]\n",
    "        \n",
    "        # Keep only neutral and happy (remove anger, sadness, fear, surprise, disgust)\n",
    "        filtered_emotions = {\n",
    "            'neutral': emotion_scores.get('neutral', 0),\n",
    "            'happy': emotion_scores.get('happy', 0)\n",
    "        }\n",
    "        \n",
    "        # Find dominant emotion\n",
    "        dominant_emotion = max(filtered_emotions, key=filtered_emotions.get)\n",
    "        dominant_confidence = filtered_emotions[dominant_emotion]\n",
    "        \n",
    "        # Detect gaze direction\n",
    "        gaze_direction = self.detect_gaze_direction(frame, face_box)\n",
    "        \n",
    "        # Calculate confidence level (SIMPLE)\n",
    "        confidence_level = self.calculate_confidence_level(gaze_direction)\n",
    "        \n",
    "        # Track sustained low confidence (only count if >1 second)\n",
    "        is_sustained_low_confidence = False\n",
    "        if confidence_level == 'low':\n",
    "            if self.low_confidence_start_time is None:\n",
    "                self.low_confidence_start_time = current_time\n",
    "            elif (current_time - self.low_confidence_start_time).total_seconds() > 1.0:\n",
    "                is_sustained_low_confidence = True\n",
    "                self.sustained_low_confidence_count += 1\n",
    "        else:\n",
    "            self.low_confidence_start_time = None\n",
    "        \n",
    "        # Smooth emotions\n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "        \n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'confidence': round(dominant_confidence, 3),\n",
    "            'gaze_direction': gaze_direction,\n",
    "            'confidence_level': confidence_level,\n",
    "            'sustained_low_confidence': is_sustained_low_confidence,\n",
    "            'filtered_emotions': {k: round(v, 3) for k, v in filtered_emotions.items()},\n",
    "            'face_box': face_box\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        return emotion_entry\n",
    "\n",
    "    def end_interview(self):\n",
    "        \"\"\"End interview and generate summary.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return None\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_summary()\n",
    "        \n",
    "        print(f\"\\n📊 Simple Interview Analysis Complete!\")\n",
    "        print(f\"Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "        return self.current_session\n",
    "\n",
    "    def generate_summary(self):\n",
    "        \"\"\"Generate simple summary.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        \n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        gaze_percentages = (df['gaze_direction'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        confidence_percentages = (df['confidence_level'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        \n",
    "        engagement_score = gaze_percentages.get('center', 0)\n",
    "        \n",
    "        low_confidence_frames = df['sustained_low_confidence'].sum()\n",
    "        fps_estimate = 30\n",
    "        low_confidence_duration = round(low_confidence_frames / fps_estimate, 2)\n",
    "        \n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'dominant_emotion_overall': max(emotion_percentages.keys(), key=lambda k: emotion_percentages[k]) if emotion_percentages else 'neutral',\n",
    "        }\n",
    "        \n",
    "        self.current_session['gaze_analysis'] = {\n",
    "            'gaze_percentages': gaze_percentages,\n",
    "            'engagement_score': round(engagement_score, 1),\n",
    "            'looking_at_camera_percentage': gaze_percentages.get('center', 0)\n",
    "        }\n",
    "        \n",
    "        self.current_session['confidence_metrics'] = {\n",
    "            'confidence_distribution': confidence_percentages,\n",
    "            'high_confidence_percentage': confidence_percentages.get('high', 0),\n",
    "            'sustained_low_confidence_duration_seconds': low_confidence_duration\n",
    "        }\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Generate simple final report.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No analysis data available.\"}\n",
    "        \n",
    "        return {\n",
    "            'session_info': {\n",
    "                'session_id': self.current_session['session_id'],\n",
    "                'duration': f\"{self.current_session['total_duration']:.1f} seconds\"\n",
    "            },\n",
    "            'emotional_state': {\n",
    "                'dominant_emotion': self.current_session['emotion_summary']['dominant_emotion_overall'].title(),\n",
    "                'emotion_breakdown': self.current_session['emotion_summary']['emotion_percentages']\n",
    "            },\n",
    "            'engagement_analysis': {\n",
    "                'looking_at_camera': f\"{self.current_session['gaze_analysis']['looking_at_camera_percentage']:.1f}%\",\n",
    "                'gaze_distribution': self.current_session['gaze_analysis']['gaze_percentages']\n",
    "            },\n",
    "            'confidence_assessment': {\n",
    "                'high_confidence_time': f\"{self.current_session['confidence_metrics']['high_confidence_percentage']:.1f}%\",\n",
    "                'low_confidence_duration': f\"{self.current_session['confidence_metrics']['sustained_low_confidence_duration_seconds']:.1f}s\",\n",
    "                'confidence_breakdown': self.current_session['confidence_metrics']['confidence_distribution']\n",
    "            }\n",
    "        }\n",
    "\n",
    "def run_simple_interview_analysis():\n",
    "    \"\"\"Main function - simple and focused.\"\"\"\n",
    "    analyzer = SimpleInterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 SIMPLE AI Interview Analysis\")\n",
    "    print(\"SIMPLE RULES:\")\n",
    "    print(\"✅ Looking at camera (CENTER) = HIGH confidence\")\n",
    "    print(\"❌ Looking left/right = LOW confidence\")  \n",
    "    print(\"📊 Emotions detected: NEUTRAL, HAPPY only\")\n",
    "    print(\"\\nControls:\")\n",
    "    print(\"Press 's' to START interview\")\n",
    "    print(\"Press 'q' to END interview\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # Save report\n",
    "                session_id = analyzer.current_session['session_id']\n",
    "                report_filename = f\"simple_interview_report_{session_id}.json\"\n",
    "                with open(report_filename, 'w') as f:\n",
    "                    json.dump(report, f, indent=4)\n",
    "                print(f\"📄 Report saved: {report_filename}\")\n",
    "\n",
    "                # Display results\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"📊 SIMPLE INTERVIEW REPORT\")\n",
    "                print(\"=\"*50)\n",
    "                print(f\"Duration: {report['session_info']['duration']}\")\n",
    "                print(f\"Dominant Emotion: {report['emotional_state']['dominant_emotion']}\")\n",
    "                print(f\"Looking at Camera: {report['engagement_analysis']['looking_at_camera']}\")\n",
    "                print(f\"High Confidence: {report['confidence_assessment']['high_confidence_time']}\")\n",
    "                print(f\"Low Confidence Duration: {report['confidence_assessment']['low_confidence_duration']}\")\n",
    "                print(\"=\"*50)\n",
    "            break\n",
    "        \n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            if emotion_data:\n",
    "                x, y, w, h = emotion_data['face_box']\n",
    "                emotion = emotion_data['dominant_emotion']\n",
    "                gaze = emotion_data['gaze_direction']\n",
    "                conf_level = emotion_data['confidence_level']\n",
    "                \n",
    "                # Simple color coding\n",
    "                color = (0, 255, 0) if conf_level == 'high' else (0, 0, 255)  # Green or Red\n",
    "                \n",
    "                # Draw rectangle and text\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{emotion.upper()}\", \n",
    "                           (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                cv2.putText(frame, f\"{conf_level.upper()}\", \n",
    "                           (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                cv2.putText(frame, f\"Gaze: {gaze.upper()}\", \n",
    "                           (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Status\n",
    "        status = \"🔴 RECORDING\" if interview_started else \"⚪ Press 's' to START\"\n",
    "        cv2.putText(frame, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, \n",
    "                   (0, 0, 255) if interview_started else (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.imshow('Simple AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_simple_interview_analysis()\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dbbe8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 INTEGRATED AI Interview Analysis\n",
      "Features:\n",
      "✅ Comprehensive emotion tracking (including sadness >0.40 threshold)\n",
      "✅ Simple gaze-based confidence (center=HIGH, left/right=LOW)\n",
      "✅ Critical moment detection\n",
      "✅ Engagement scoring\n",
      "\n",
      "Controls:\n",
      "Press 's' to START interview\n",
      "Press 'q' to END interview and generate comprehensive report\n",
      "🎥 Enhanced Interview Analysis Started: 20250906_102925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(2, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Enhanced Interview Analysis Complete!\n",
      "Duration: 25.2s\n",
      "📄 Comprehensive report saved: integrated_interview_report_20250906_102925.json\n",
      "\n",
      "======================================================================\n",
      "📊 INTEGRATED INTERVIEW ANALYSIS REPORT\n",
      "======================================================================\n",
      "Overall Assessment: Excellent\n",
      "Emotional Stability: 99.3%\n",
      "Dominant Emotion: Neutral\n",
      "Engagement Score: 99.3%\n",
      "Looking at Camera: 99.3%\n",
      "Gaze Low Confidence Duration: 0.0s\n",
      "\n",
      "📈 Detailed Breakdowns:\n",
      "Emotions: {'neutral': 100.0}\n",
      "Gaze Distribution: {'center': 99.27, 'right': 0.73}\n",
      "Gaze Confidence: {'high': 99.27, 'low': 0.73}\n",
      "Critical Moments: {'prolonged_neutrality': 1}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "## Yeh one of teh final hai\n",
    "\n",
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "\n",
    "class IntegratedInterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive emotion analyzer that combines:\n",
    "    - Original emotion tracking with critical moments\n",
    "    - Simple gaze-based confidence (center=high, left/right=low)\n",
    "    - Sadness detection with appropriate thresholds\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Original tracking variables\n",
    "        self.emotion_history = deque(maxlen=15)\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        \n",
    "        # NEW: Gaze tracking variables\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "        self.gaze_low_confidence_start = None\n",
    "        self.sustained_gaze_low_confidence_count = 0\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'gaze_analysis': {},  # NEW: Gaze analysis\n",
    "            'critical_moments': []\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        # Reset all tracking variables for a clean start\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        \n",
    "        # NEW: Reset gaze tracking\n",
    "        self.gaze_low_confidence_start = None\n",
    "        self.sustained_gaze_low_confidence_count = 0\n",
    "        \n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Enhanced Interview Analysis Started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def detect_gaze_direction(self, frame, face_box):\n",
    "        \"\"\"\n",
    "        Simple gaze detection: center, left, right\n",
    "        Returns: 'center', 'left', 'right'\n",
    "        \"\"\"\n",
    "        x, y, w, h = face_box\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        eyes = self.eye_cascade.detectMultiScale(face_gray, 1.1, 5)\n",
    "        \n",
    "        if len(eyes) < 2:\n",
    "            return 'center'  # Default to center\n",
    "        \n",
    "        face_center_x = w // 2\n",
    "        eyes_sorted = sorted(eyes, key=lambda e: e[2] * e[3], reverse=True)[:2]\n",
    "        \n",
    "        eye_centers_x = []\n",
    "        for (ex, ey, ew, eh) in eyes_sorted:\n",
    "            eye_center_x = ex + ew // 2\n",
    "            eye_centers_x.append(eye_center_x)\n",
    "        \n",
    "        if len(eye_centers_x) == 2:\n",
    "            avg_eye_x = sum(eye_centers_x) // 2\n",
    "            horizontal_deviation = avg_eye_x - face_center_x\n",
    "            threshold = w * 0.15  # 15% of face width\n",
    "            \n",
    "            if abs(horizontal_deviation) <= threshold:\n",
    "                return 'center'\n",
    "            elif horizontal_deviation < -threshold:\n",
    "                return 'left'\n",
    "            else:\n",
    "                return 'right'\n",
    "        \n",
    "        return 'center'\n",
    "\n",
    "    def calculate_gaze_confidence(self, gaze_direction):\n",
    "        \"\"\"\n",
    "        Simple gaze-based confidence:\n",
    "        - Center = HIGH confidence  \n",
    "        - Left/Right = LOW confidence\n",
    "        \"\"\"\n",
    "        if gaze_direction == 'center':\n",
    "            return 'high'\n",
    "        elif gaze_direction in ['left', 'right']:\n",
    "            return 'low'\n",
    "        else:\n",
    "            return 'high'  # Default\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Enhanced frame analysis combining original emotion tracking with gaze analysis.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        face_box = primary_face[\"box\"]\n",
    "        \n",
    "        # Original emotion processing\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "\n",
    "        # NEW: Gaze-based confidence\n",
    "        gaze_direction = self.detect_gaze_direction(frame, face_box)\n",
    "        gaze_confidence_level = self.calculate_gaze_confidence(gaze_direction)\n",
    "        \n",
    "        # Track sustained gaze-based low confidence\n",
    "        is_sustained_gaze_low_confidence = False\n",
    "        if gaze_confidence_level == 'low':\n",
    "            if self.gaze_low_confidence_start is None:\n",
    "                self.gaze_low_confidence_start = current_time\n",
    "            elif (current_time - self.gaze_low_confidence_start).total_seconds() > 1.0:\n",
    "                is_sustained_gaze_low_confidence = True\n",
    "                self.sustained_gaze_low_confidence_count += 1\n",
    "        else:\n",
    "            self.gaze_low_confidence_start = None\n",
    "\n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "            'face_box': face_box,\n",
    "            # NEW: Gaze data\n",
    "            'gaze_direction': gaze_direction,\n",
    "            'gaze_confidence_level': gaze_confidence_level,\n",
    "            'sustained_gaze_low_confidence': is_sustained_gaze_low_confidence\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        self.detect_critical_moments(emotion_entry)\n",
    "        return emotion_entry\n",
    "\n",
    "    def detect_critical_moments(self, emotion_entry):\n",
    "        \"\"\"\n",
    "        Enhanced critical moment detection including gaze-based issues.\n",
    "        \"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        all_emotions = emotion_entry['all_emotions']\n",
    "        gaze_direction = emotion_entry['gaze_direction']\n",
    "\n",
    "        # Original emotion-based detection\n",
    "        if (emotion == 'sad' and confidence > 0.40) or (emotion == 'fear' and confidence > 0.4):\n",
    "            self._log_critical_moment('high_stress', emotion_entry, f\"Candidate showed strong signs of {emotion}.\")\n",
    "\n",
    "        if emotion == 'neutral':\n",
    "            self.neutral_streak += 1\n",
    "            if self.neutral_streak == 30:\n",
    "                self._log_critical_moment('prolonged_neutrality', emotion_entry, \"Candidate showed a prolonged neutral expression.\")\n",
    "        else:\n",
    "            self.neutral_streak = 0\n",
    "\n",
    "        # Original low confidence detection\n",
    "        is_ambiguous = (all_emotions.get('neutral', 0) > 0.2 and (all_emotions.get('sad', 0) + all_emotions.get('fear', 0)) > 0.25)\n",
    "        if confidence < 0.45 and is_ambiguous:\n",
    "            self.low_confidence_streak += 1\n",
    "            if self.low_confidence_streak == 15:\n",
    "                self._log_critical_moment('low_confidence', emotion_entry, \"Candidate appeared uncertain.\")\n",
    "        else:\n",
    "            self.low_confidence_streak = 0\n",
    "\n",
    "        # NEW: Gaze-based critical moments\n",
    "        if gaze_direction in ['left', 'right'] and emotion_entry['sustained_gaze_low_confidence']:\n",
    "            self._log_critical_moment('gaze_avoidance', emotion_entry, f\"Candidate avoided eye contact by looking {gaze_direction}.\")\n",
    "\n",
    "    def _log_critical_moment(self, type, entry, description):\n",
    "        \"\"\"Helper to append a critical moment to the session log, avoiding rapid duplicates.\"\"\"\n",
    "        if self.current_session['critical_moments']:\n",
    "            last_moment = self.current_session['critical_moments'][-1]\n",
    "            if last_moment['type'] == type and (entry['elapsed_seconds'] - last_moment['elapsed_seconds']) < 5:\n",
    "                return\n",
    "        self.current_session['critical_moments'].append({\n",
    "            'type': type, 'timestamp': entry['timestamp'], 'elapsed_seconds': entry['elapsed_seconds'],\n",
    "            'emotion': entry['dominant_emotion'], 'confidence': entry['confidence'], \n",
    "            'gaze_direction': entry.get('gaze_direction', 'unknown'),\n",
    "            'description': description\n",
    "        })\n",
    "        \n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates comprehensive summary.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_comprehensive_summary()\n",
    "        \n",
    "        print(f\"\\n📊 Enhanced Interview Analysis Complete!\")\n",
    "        print(f\"Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "\n",
    "    def generate_comprehensive_summary(self):\n",
    "        \"\"\"Enhanced summary generation including gaze analysis.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        \n",
    "        # Original emotion analysis\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        emotion_changes = (df['dominant_emotion'].shift() != df['dominant_emotion']).sum()\n",
    "        stability_score = max(0, 100 - (emotion_changes / len(df) * 100)) if len(df) > 0 else 100\n",
    "        \n",
    "        # NEW: Gaze analysis\n",
    "        gaze_percentages = (df['gaze_direction'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        gaze_confidence_percentages = (df['gaze_confidence_level'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        engagement_score = gaze_percentages.get('center', 0)\n",
    "        \n",
    "        # Calculate gaze-based low confidence duration\n",
    "        gaze_low_confidence_frames = df['sustained_gaze_low_confidence'].sum()\n",
    "        fps_estimate = 30\n",
    "        gaze_low_confidence_duration = round(gaze_low_confidence_frames / fps_estimate, 2)\n",
    "        \n",
    "        critical_moment_counts = defaultdict(int)\n",
    "        for moment in self.current_session['critical_moments']:\n",
    "            critical_moment_counts[moment['type']] += 1\n",
    "\n",
    "        # Original emotion summary\n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages, key=emotion_percentages.get, default='neutral'),\n",
    "            'critical_moment_counts': dict(critical_moment_counts)\n",
    "        }\n",
    "        \n",
    "        # NEW: Gaze analysis summary\n",
    "        self.current_session['gaze_analysis'] = {\n",
    "            'gaze_percentages': gaze_percentages,\n",
    "            'gaze_confidence_percentages': gaze_confidence_percentages,\n",
    "            'engagement_score': round(engagement_score, 1),\n",
    "            'looking_at_camera_percentage': gaze_percentages.get('center', 0),\n",
    "            'gaze_low_confidence_duration_seconds': gaze_low_confidence_duration\n",
    "        }\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Enhanced final report including gaze analysis.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No summary available.\"}\n",
    "        \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        gaze_summary = self.current_session.get('gaze_analysis', {})\n",
    "        \n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary.get('critical_moment_counts', {}).values())\n",
    "        engagement = gaze_summary.get('engagement_score', 0)\n",
    "\n",
    "        # Enhanced assessment including gaze\n",
    "        if stability >= 75 and stress_total <= 2 and engagement >= 70:\n",
    "            assessment = \"Excellent\"\n",
    "        elif stability >= 55 and stress_total <= 5 and engagement >= 50:\n",
    "            assessment = \"Good\"\n",
    "        elif stability >= 35 and engagement >= 30:\n",
    "            assessment = \"Fair\"\n",
    "        else:\n",
    "            assessment = \"Needs Attention\"\n",
    "            \n",
    "        return {\n",
    "            'assessment': assessment,\n",
    "            'stability_score': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "            'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "            'emotion_breakdown_percentage': summary['emotion_percentages'],\n",
    "            'engagement_score': f\"{gaze_summary.get('engagement_score', 0):.1f}%\",\n",
    "            'looking_at_camera_percentage': f\"{gaze_summary.get('looking_at_camera_percentage', 0):.1f}%\",\n",
    "            'gaze_distribution': gaze_summary.get('gaze_percentages', {}),\n",
    "            'gaze_confidence_breakdown': gaze_summary.get('gaze_confidence_percentages', {}),\n",
    "            'gaze_low_confidence_duration': f\"{gaze_summary.get('gaze_low_confidence_duration_seconds', 0):.1f}s\",\n",
    "            'critical_moments_summary': dict(summary.get('critical_moment_counts', {}))\n",
    "        }\n",
    "\n",
    "def run_integrated_interview_analysis():\n",
    "    \"\"\"Main loop combining comprehensive emotion analysis with simple gaze confidence.\"\"\"\n",
    "    analyzer = IntegratedInterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 INTEGRATED AI Interview Analysis\")\n",
    "    print(\"Features:\")\n",
    "    print(\"✅ Comprehensive emotion tracking (including sadness >0.40 threshold)\")\n",
    "    print(\"✅ Simple gaze-based confidence (center=HIGH, left/right=LOW)\")\n",
    "    print(\"✅ Critical moment detection\")\n",
    "    print(\"✅ Engagement scoring\")\n",
    "    print(\"\\nControls:\")\n",
    "    print(\"Press 's' to START interview\")\n",
    "    print(\"Press 'q' to END interview and generate comprehensive report\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # Save comprehensive report\n",
    "                if 'error' not in report:\n",
    "                    session_id = analyzer.current_session['session_id']\n",
    "                    report_filename = f\"integrated_interview_report_{session_id}.json\"\n",
    "                    with open(report_filename, 'w') as f:\n",
    "                        json.dump(report, f, indent=4)\n",
    "                    print(f\"📄 Comprehensive report saved: {report_filename}\")\n",
    "\n",
    "                # Display enhanced report\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"📊 INTEGRATED INTERVIEW ANALYSIS REPORT\")\n",
    "                print(\"=\"*70)\n",
    "                print(f\"Overall Assessment: {report.get('assessment', 'N/A')}\")\n",
    "                print(f\"Emotional Stability: {report.get('stability_score', 'N/A')}\")\n",
    "                print(f\"Dominant Emotion: {report.get('dominant_emotion', 'N/A')}\")\n",
    "                print(f\"Engagement Score: {report.get('engagement_score', 'N/A')}\")\n",
    "                print(f\"Looking at Camera: {report.get('looking_at_camera_percentage', 'N/A')}\")\n",
    "                print(f\"Gaze Low Confidence Duration: {report.get('gaze_low_confidence_duration', 'N/A')}\")\n",
    "                \n",
    "                print(f\"\\n📈 Detailed Breakdowns:\")\n",
    "                print(f\"Emotions: {report.get('emotion_breakdown_percentage', {})}\")\n",
    "                print(f\"Gaze Distribution: {report.get('gaze_distribution', {})}\")\n",
    "                print(f\"Gaze Confidence: {report.get('gaze_confidence_breakdown', {})}\")\n",
    "                print(f\"Critical Moments: {report.get('critical_moments_summary', {})}\")\n",
    "                print(\"=\"*70)\n",
    "            break\n",
    "        \n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            if emotion_data:\n",
    "                x, y, w, h = emotion_data['face_box']\n",
    "                emotion = emotion_data['dominant_emotion']\n",
    "                confidence = emotion_data['confidence']\n",
    "                gaze = emotion_data['gaze_direction']\n",
    "                gaze_conf = emotion_data['gaze_confidence_level']\n",
    "                \n",
    "                # Color based on gaze confidence (simple logic)\n",
    "                if gaze_conf == 'high':\n",
    "                    color = (0, 255, 0)  # Green for high confidence\n",
    "                else:\n",
    "                    color = (0, 0, 255)  # Red for low confidence\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"{emotion.upper()} ({confidence:.2f})\", \n",
    "                            (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                cv2.putText(frame, f\"Gaze: {gaze.upper()} - {gaze_conf.upper()}\", \n",
    "                            (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Status display\n",
    "        status_text = \"🔴 RECORDING\" if interview_started else \"⚪ READY - Press 's' to START\"\n",
    "        status_color = (0, 0, 255) if interview_started else (0, 255, 0)\n",
    "        cv2.putText(frame, status_text, (10, frame.shape[0] - 15), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)\n",
    "        \n",
    "        cv2.imshow('Integrated AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_integrated_interview_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d56ae422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎤 CLEAN AI Interview Analysis\n",
      "Features:\n",
      "✅ Comprehensive emotion tracking (including sadness >0.40 threshold)\n",
      "✅ Simple gaze-based confidence (center=HIGH, left/right=LOW)\n",
      "✅ Critical moment detection\n",
      "✅ Engagement scoring\n",
      "\n",
      "Controls:\n",
      "Press 's' to START interview\n",
      "Press 'q' to END interview and generate comprehensive report\n",
      "🎥 Enhanced Interview Analysis Started: 20250906_230134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_1']\n",
      "Received: inputs=Tensor(shape=(1, 64, 64))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Enhanced Interview Analysis Complete!\n",
      "Duration: 8.3s\n",
      "📄 Comprehensive report saved: clean_interview_report_20250906_230134.json\n",
      "\n",
      "======================================================================\n",
      "📊 CLEAN INTERVIEW ANALYSIS REPORT\n",
      "======================================================================\n",
      "Overall Assessment: Excellent\n",
      "Emotional Stability: 87.8%\n",
      "Dominant Emotion: Angry\n",
      "Engagement Score: 100.0%\n",
      "Looking at Camera: 100.0%\n",
      "Gaze Low Confidence Duration: 0.0s\n",
      "\n",
      "📈 Detailed Breakdowns:\n",
      "Emotions: {'angry': 93.88, 'happy': 6.12}\n",
      "Gaze Distribution: {'center': 100.0}\n",
      "Gaze Confidence: {'high': 100.0}\n",
      "Critical Moments: {}\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "\n",
    "class IntegratedInterviewEmotionAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive emotion analyzer that combines:\n",
    "    - Original emotion tracking with critical moments\n",
    "    - Simple gaze-based confidence (center=high, left/right=low)\n",
    "    - Sadness detection with appropriate thresholds\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Original tracking variables\n",
    "        self.emotion_history = deque(maxlen=15)\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        \n",
    "        # NEW: Gaze tracking variables\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "        self.gaze_low_confidence_start = None\n",
    "        self.sustained_gaze_low_confidence_count = 0\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'gaze_analysis': {},  # NEW: Gaze analysis\n",
    "            'critical_moments': []\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        # Reset all tracking variables for a clean start\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        \n",
    "        # NEW: Reset gaze tracking\n",
    "        self.gaze_low_confidence_start = None\n",
    "        self.sustained_gaze_low_confidence_count = 0\n",
    "        \n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        print(f\"🎥 Enhanced Interview Analysis Started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def detect_gaze_direction(self, frame, face_box):\n",
    "        \"\"\"\n",
    "        Simple gaze detection: center, left, right\n",
    "        Returns: 'center', 'left', 'right'\n",
    "        \"\"\"\n",
    "        x, y, w, h = face_box\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        eyes = self.eye_cascade.detectMultiScale(face_gray, 1.1, 5)\n",
    "        \n",
    "        if len(eyes) < 2:\n",
    "            return 'center'  # Default to center\n",
    "        \n",
    "        face_center_x = w // 2\n",
    "        eyes_sorted = sorted(eyes, key=lambda e: e[2] * e[3], reverse=True)[:2]\n",
    "        \n",
    "        eye_centers_x = []\n",
    "        for (ex, ey, ew, eh) in eyes_sorted:\n",
    "            eye_center_x = ex + ew // 2\n",
    "            eye_centers_x.append(eye_center_x)\n",
    "        \n",
    "        if len(eye_centers_x) == 2:\n",
    "            avg_eye_x = sum(eye_centers_x) // 2\n",
    "            horizontal_deviation = avg_eye_x - face_center_x\n",
    "            threshold = w * 0.15  # 15% of face width\n",
    "            \n",
    "            if abs(horizontal_deviation) <= threshold:\n",
    "                return 'center'\n",
    "            elif horizontal_deviation < -threshold:\n",
    "                return 'left'\n",
    "            else:\n",
    "                return 'right'\n",
    "        \n",
    "        return 'center'\n",
    "\n",
    "    def calculate_gaze_confidence(self, gaze_direction):\n",
    "        \"\"\"\n",
    "        Simple gaze-based confidence:\n",
    "        - Center = HIGH confidence  \n",
    "        - Left/Right = LOW confidence\n",
    "        \"\"\"\n",
    "        if gaze_direction == 'center':\n",
    "            return 'high'\n",
    "        elif gaze_direction in ['left', 'right']:\n",
    "            return 'low'\n",
    "        else:\n",
    "            return 'high'  # Default\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Enhanced frame analysis combining original emotion tracking with gaze analysis.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        face_box = primary_face[\"box\"]\n",
    "        \n",
    "        # Original emotion processing\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "\n",
    "        # NEW: Gaze-based confidence\n",
    "        gaze_direction = self.detect_gaze_direction(frame, face_box)\n",
    "        gaze_confidence_level = self.calculate_gaze_confidence(gaze_direction)\n",
    "        \n",
    "        # Track sustained gaze-based low confidence\n",
    "        is_sustained_gaze_low_confidence = False\n",
    "        if gaze_confidence_level == 'low':\n",
    "            if self.gaze_low_confidence_start is None:\n",
    "                self.gaze_low_confidence_start = current_time\n",
    "            elif (current_time - self.gaze_low_confidence_start).total_seconds() > 1.0:\n",
    "                is_sustained_gaze_low_confidence = True\n",
    "                self.sustained_gaze_low_confidence_count += 1\n",
    "        else:\n",
    "            self.gaze_low_confidence_start = None\n",
    "\n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "            'face_box': face_box,\n",
    "            # NEW: Gaze data\n",
    "            'gaze_direction': gaze_direction,\n",
    "            'gaze_confidence_level': gaze_confidence_level,\n",
    "            'sustained_gaze_low_confidence': is_sustained_gaze_low_confidence\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        self.detect_critical_moments(emotion_entry)\n",
    "        return emotion_entry\n",
    "\n",
    "    def detect_critical_moments(self, emotion_entry):\n",
    "        \"\"\"\n",
    "        Enhanced critical moment detection including gaze-based issues.\n",
    "        \"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        all_emotions = emotion_entry['all_emotions']\n",
    "        gaze_direction = emotion_entry['gaze_direction']\n",
    "\n",
    "        # Original emotion-based detection\n",
    "        if (emotion == 'sad' and confidence > 0.40) or (emotion == 'fear' and confidence > 0.4):\n",
    "            self._log_critical_moment('high_stress', emotion_entry, f\"Candidate showed strong signs of {emotion}.\")\n",
    "\n",
    "        if emotion == 'neutral':\n",
    "            self.neutral_streak += 1\n",
    "            if self.neutral_streak == 30:\n",
    "                self._log_critical_moment('prolonged_neutrality', emotion_entry, \"Candidate showed a prolonged neutral expression.\")\n",
    "        else:\n",
    "            self.neutral_streak = 0\n",
    "\n",
    "        # Original low confidence detection\n",
    "        is_ambiguous = (all_emotions.get('neutral', 0) > 0.2 and (all_emotions.get('sad', 0) + all_emotions.get('fear', 0)) > 0.25)\n",
    "        if confidence < 0.45 and is_ambiguous:\n",
    "            self.low_confidence_streak += 1\n",
    "            if self.low_confidence_streak == 15:\n",
    "                self._log_critical_moment('low_confidence', emotion_entry, \"Candidate appeared uncertain.\")\n",
    "        else:\n",
    "            self.low_confidence_streak = 0\n",
    "\n",
    "        # NEW: Gaze-based critical moments\n",
    "        if gaze_direction in ['left', 'right'] and emotion_entry['sustained_gaze_low_confidence']:\n",
    "            self._log_critical_moment('gaze_avoidance', emotion_entry, f\"Candidate avoided eye contact by looking {gaze_direction}.\")\n",
    "\n",
    "    def _log_critical_moment(self, type, entry, description):\n",
    "        \"\"\"Helper to append a critical moment to the session log, avoiding rapid duplicates.\"\"\"\n",
    "        if self.current_session['critical_moments']:\n",
    "            last_moment = self.current_session['critical_moments'][-1]\n",
    "            if last_moment['type'] == type and (entry['elapsed_seconds'] - last_moment['elapsed_seconds']) < 5:\n",
    "                return\n",
    "        self.current_session['critical_moments'].append({\n",
    "            'type': type, 'timestamp': entry['timestamp'], 'elapsed_seconds': entry['elapsed_seconds'],\n",
    "            'emotion': entry['dominant_emotion'], 'confidence': entry['confidence'], \n",
    "            'gaze_direction': entry.get('gaze_direction', 'unknown'),\n",
    "            'description': description\n",
    "        })\n",
    "        \n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates comprehensive summary.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            print(\"No interview session was started.\")\n",
    "            return\n",
    "            \n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_comprehensive_summary()\n",
    "        \n",
    "        print(f\"\\n📊 Enhanced Interview Analysis Complete!\")\n",
    "        print(f\"Duration: {self.current_session['total_duration']:.1f}s\")\n",
    "\n",
    "    def generate_comprehensive_summary(self):\n",
    "        \"\"\"Enhanced summary generation including gaze analysis.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        \n",
    "        # Original emotion analysis\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        emotion_changes = (df['dominant_emotion'].shift() != df['dominant_emotion']).sum()\n",
    "        stability_score = max(0, 100 - (emotion_changes / len(df) * 100)) if len(df) > 0 else 100\n",
    "        \n",
    "        # NEW: Gaze analysis\n",
    "        gaze_percentages = (df['gaze_direction'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        gaze_confidence_percentages = (df['gaze_confidence_level'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        engagement_score = gaze_percentages.get('center', 0)\n",
    "        \n",
    "        # Calculate gaze-based low confidence duration\n",
    "        gaze_low_confidence_frames = df['sustained_gaze_low_confidence'].sum()\n",
    "        fps_estimate = 30\n",
    "        gaze_low_confidence_duration = round(gaze_low_confidence_frames / fps_estimate, 2)\n",
    "        \n",
    "        critical_moment_counts = defaultdict(int)\n",
    "        for moment in self.current_session['critical_moments']:\n",
    "            critical_moment_counts[moment['type']] += 1\n",
    "\n",
    "        # Original emotion summary\n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages, key=emotion_percentages.get, default='neutral'),\n",
    "            'critical_moment_counts': dict(critical_moment_counts)\n",
    "        }\n",
    "        \n",
    "        # NEW: Gaze analysis summary\n",
    "        self.current_session['gaze_analysis'] = {\n",
    "            'gaze_percentages': gaze_percentages,\n",
    "            'gaze_confidence_percentages': gaze_confidence_percentages,\n",
    "            'engagement_score': round(engagement_score, 1),\n",
    "            'looking_at_camera_percentage': gaze_percentages.get('center', 0),\n",
    "            'gaze_low_confidence_duration_seconds': gaze_low_confidence_duration\n",
    "        }\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Enhanced final report including gaze analysis.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No summary available.\"}\n",
    "        \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        gaze_summary = self.current_session.get('gaze_analysis', {})\n",
    "        \n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary.get('critical_moment_counts', {}).values())\n",
    "        engagement = gaze_summary.get('engagement_score', 0)\n",
    "\n",
    "        # Enhanced assessment including gaze\n",
    "        if stability >= 75 and stress_total <= 2 and engagement >= 70:\n",
    "            assessment = \"Excellent\"\n",
    "        elif stability >= 55 and stress_total <= 5 and engagement >= 50:\n",
    "            assessment = \"Good\"\n",
    "        elif stability >= 35 and engagement >= 30:\n",
    "            assessment = \"Fair\"\n",
    "        else:\n",
    "            assessment = \"Needs Attention\"\n",
    "            \n",
    "        return {\n",
    "            'assessment': assessment,\n",
    "            'stability_score': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "            'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "            'emotion_breakdown_percentage': summary['emotion_percentages'],\n",
    "            'engagement_score': f\"{gaze_summary.get('engagement_score', 0):.1f}%\",\n",
    "            'looking_at_camera_percentage': f\"{gaze_summary.get('looking_at_camera_percentage', 0):.1f}%\",\n",
    "            'gaze_distribution': gaze_summary.get('gaze_percentages', {}),\n",
    "            'gaze_confidence_breakdown': gaze_summary.get('gaze_confidence_percentages', {}),\n",
    "            'gaze_low_confidence_duration': f\"{gaze_summary.get('gaze_low_confidence_duration_seconds', 0):.1f}s\",\n",
    "            'critical_moments_summary': dict(summary.get('critical_moment_counts', {}))\n",
    "        }\n",
    "\n",
    "def run_integrated_interview_analysis():\n",
    "    \"\"\"Main loop combining comprehensive emotion analysis with simple gaze confidence.\"\"\"\n",
    "    analyzer = IntegratedInterviewEmotionAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🎤 CLEAN AI Interview Analysis\")\n",
    "    print(\"Features:\")\n",
    "    print(\"✅ Comprehensive emotion tracking (including sadness >0.40 threshold)\")\n",
    "    print(\"✅ Simple gaze-based confidence (center=HIGH, left/right=LOW)\")\n",
    "    print(\"✅ Critical moment detection\")\n",
    "    print(\"✅ Engagement scoring\")\n",
    "    print(\"\\nControls:\")\n",
    "    print(\"Press 's' to START interview\")\n",
    "    print(\"Press 'q' to END interview and generate comprehensive report\")\n",
    "    \n",
    "    interview_started = False\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            analyzer.start_interview()\n",
    "            interview_started = True\n",
    "        \n",
    "        elif key == ord('q'):\n",
    "            if interview_started:\n",
    "                analyzer.end_interview()\n",
    "                report = analyzer.get_final_report()\n",
    "                \n",
    "                # Save comprehensive report\n",
    "                if 'error' not in report:\n",
    "                    session_id = analyzer.current_session['session_id']\n",
    "                    report_filename = f\"clean_interview_report_{session_id}.json\"\n",
    "                    with open(report_filename, 'w') as f:\n",
    "                        json.dump(report, f, indent=4)\n",
    "                    print(f\"📄 Comprehensive report saved: {report_filename}\")\n",
    "\n",
    "                # Display enhanced report\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"📊 CLEAN INTERVIEW ANALYSIS REPORT\")\n",
    "                print(\"=\"*70)\n",
    "                print(f\"Overall Assessment: {report.get('assessment', 'N/A')}\")\n",
    "                print(f\"Emotional Stability: {report.get('stability_score', 'N/A')}\")\n",
    "                print(f\"Dominant Emotion: {report.get('dominant_emotion', 'N/A')}\")\n",
    "                print(f\"Engagement Score: {report.get('engagement_score', 'N/A')}\")\n",
    "                print(f\"Looking at Camera: {report.get('looking_at_camera_percentage', 'N/A')}\")\n",
    "                print(f\"Gaze Low Confidence Duration: {report.get('gaze_low_confidence_duration', 'N/A')}\")\n",
    "                \n",
    "                print(f\"\\n📈 Detailed Breakdowns:\")\n",
    "                print(f\"Emotions: {report.get('emotion_breakdown_percentage', {})}\")\n",
    "                print(f\"Gaze Distribution: {report.get('gaze_distribution', {})}\")\n",
    "                print(f\"Gaze Confidence: {report.get('gaze_confidence_breakdown', {})}\")\n",
    "                print(f\"Critical Moments: {report.get('critical_moments_summary', {})}\")\n",
    "                print(\"=\"*70)\n",
    "            break\n",
    "        \n",
    "        if interview_started:\n",
    "            emotion_data = analyzer.analyze_frame(frame)\n",
    "            # REMOVED: All visual overlays (green box, text, status)\n",
    "            # The analysis still runs in the background but shows clean video feed\n",
    "        \n",
    "        # REMOVED: Status display text (\"RECORDING\" etc.)\n",
    "        cv2.imshow('Clean AI Interview Analysis', frame)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_integrated_interview_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2680b73",
   "metadata": {},
   "source": [
    "## Websocket vaala code for integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e4d5b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting WebSocket Interview Analysis Server on ws://localhost:8765\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 434\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Starting WebSocket Interview Analysis Server on ws://localhost:8765\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    433\u001b[0m start_server \u001b[38;5;241m=\u001b[39m websockets\u001b[38;5;241m.\u001b[39mserve(handle_client, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m8765\u001b[39m)\n\u001b[1;32m--> 434\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mrun_forever()\n",
      "File \u001b[1;32mc:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py:696\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \n\u001b[0;32m    687\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[0;32m    699\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py:632\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    635\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "import cv2\n",
    "import base64\n",
    "import numpy as np\n",
    "import json\n",
    "from fer import FER\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class WebSocketInterviewAnalyzer:\n",
    "    \"\"\"\n",
    "    Complete WebSocket version of your IntegratedInterviewEmotionAnalyzer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.detector = FER(mtcnn=True)\n",
    "        self.emotion_log = []\n",
    "        self.start_time = None\n",
    "        self.current_session = self._create_new_session()\n",
    "        \n",
    "        # Original tracking variables\n",
    "        self.emotion_history = deque(maxlen=15)\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        \n",
    "        # Gaze tracking variables\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "        self.gaze_low_confidence_start = None\n",
    "        self.sustained_gaze_low_confidence_count = 0\n",
    "        \n",
    "        # WebSocket specific\n",
    "        self.clients = set()\n",
    "        self.interview_active = False\n",
    "\n",
    "    def _create_new_session(self):\n",
    "        \"\"\"Initializes a fresh session dictionary.\"\"\"\n",
    "        return {\n",
    "            'session_id': datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration': 0,\n",
    "            'emotion_timeline': [],\n",
    "            'emotion_summary': {},\n",
    "            'gaze_analysis': {},\n",
    "            'critical_moments': []\n",
    "        }\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Starts or restarts the interview analysis session.\"\"\"\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        # Reset all tracking variables for a clean start\n",
    "        self.emotion_log = []\n",
    "        self.emotion_history.clear()\n",
    "        self.neutral_streak = 0\n",
    "        self.low_confidence_streak = 0\n",
    "        \n",
    "        # Reset gaze tracking\n",
    "        self.gaze_low_confidence_start = None\n",
    "        self.sustained_gaze_low_confidence_count = 0\n",
    "        \n",
    "        self.current_session = self._create_new_session()\n",
    "        self.current_session['start_time'] = self.start_time.isoformat()\n",
    "        self.interview_active = True\n",
    "        print(f\"🎥 WebSocket Interview Analysis Started: {self.current_session['session_id']}\")\n",
    "\n",
    "    def detect_gaze_direction(self, frame, face_box):\n",
    "        \"\"\"\n",
    "        Simple gaze detection: center, left, right\n",
    "        Returns: 'center', 'left', 'right'\n",
    "        \"\"\"\n",
    "        x, y, w, h = face_box\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        face_gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        eyes = self.eye_cascade.detectMultiScale(face_gray, 1.1, 5)\n",
    "        \n",
    "        if len(eyes) < 2:\n",
    "            return 'center'  # Default to center\n",
    "        \n",
    "        face_center_x = w // 2\n",
    "        eyes_sorted = sorted(eyes, key=lambda e: e[2] * e[3], reverse=True)[:2]\n",
    "        \n",
    "        eye_centers_x = []\n",
    "        for (ex, ey, ew, eh) in eyes_sorted:\n",
    "            eye_center_x = ex + ew // 2\n",
    "            eye_centers_x.append(eye_center_x)\n",
    "        \n",
    "        if len(eye_centers_x) == 2:\n",
    "            avg_eye_x = sum(eye_centers_x) // 2\n",
    "            horizontal_deviation = avg_eye_x - face_center_x\n",
    "            threshold = w * 0.15  # 15% of face width\n",
    "            \n",
    "            if abs(horizontal_deviation) <= threshold:\n",
    "                return 'center'\n",
    "            elif horizontal_deviation < -threshold:\n",
    "                return 'left'\n",
    "            else:\n",
    "                return 'right'\n",
    "        \n",
    "        return 'center'\n",
    "\n",
    "    def calculate_gaze_confidence(self, gaze_direction):\n",
    "        \"\"\"\n",
    "        Simple gaze-based confidence:\n",
    "        - Center = HIGH confidence  \n",
    "        - Left/Right = LOW confidence\n",
    "        \"\"\"\n",
    "        if gaze_direction == 'center':\n",
    "            return 'high'\n",
    "        elif gaze_direction in ['left', 'right']:\n",
    "            return 'low'\n",
    "        else:\n",
    "            return 'high'  # Default\n",
    "\n",
    "    def analyze_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Enhanced frame analysis combining original emotion tracking with gaze analysis.\n",
    "        \"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "\n",
    "        current_time = datetime.datetime.now()\n",
    "        elapsed_seconds = (current_time - self.start_time).total_seconds()\n",
    "        \n",
    "        emotions = self.detector.detect_emotions(frame)\n",
    "        \n",
    "        if not emotions:\n",
    "            return None\n",
    "\n",
    "        primary_face = emotions[0]\n",
    "        emotion_scores = primary_face[\"emotions\"]\n",
    "        face_box = primary_face[\"box\"]\n",
    "        \n",
    "        # Original emotion processing\n",
    "        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "        confidence = emotion_scores[dominant_emotion]\n",
    "        \n",
    "        self.emotion_history.append(dominant_emotion)\n",
    "        smoothed_emotion = max(set(self.emotion_history), key=list(self.emotion_history).count)\n",
    "\n",
    "        # Gaze-based confidence\n",
    "        gaze_direction = self.detect_gaze_direction(frame, face_box)\n",
    "        gaze_confidence_level = self.calculate_gaze_confidence(gaze_direction)\n",
    "        \n",
    "        # Track sustained gaze-based low confidence\n",
    "        is_sustained_gaze_low_confidence = False\n",
    "        if gaze_confidence_level == 'low':\n",
    "            if self.gaze_low_confidence_start is None:\n",
    "                self.gaze_low_confidence_start = current_time\n",
    "            elif (current_time - self.gaze_low_confidence_start).total_seconds() > 1.0:\n",
    "                is_sustained_gaze_low_confidence = True\n",
    "                self.sustained_gaze_low_confidence_count += 1\n",
    "        else:\n",
    "            self.gaze_low_confidence_start = None\n",
    "\n",
    "        emotion_entry = {\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'elapsed_seconds': round(elapsed_seconds, 2),\n",
    "            'dominant_emotion': smoothed_emotion,\n",
    "            'confidence': round(confidence, 3),\n",
    "            'all_emotions': {k: round(v, 3) for k, v in emotion_scores.items()},\n",
    "            'face_box': face_box,\n",
    "            'gaze_direction': gaze_direction,\n",
    "            'gaze_confidence_level': gaze_confidence_level,\n",
    "            'sustained_gaze_low_confidence': is_sustained_gaze_low_confidence\n",
    "        }\n",
    "        \n",
    "        self.emotion_log.append(emotion_entry)\n",
    "        self.detect_critical_moments(emotion_entry)\n",
    "        return emotion_entry\n",
    "\n",
    "    def detect_critical_moments(self, emotion_entry):\n",
    "        \"\"\"\n",
    "        Enhanced critical moment detection including gaze-based issues.\n",
    "        \"\"\"\n",
    "        emotion = emotion_entry['dominant_emotion']\n",
    "        confidence = emotion_entry['confidence']\n",
    "        all_emotions = emotion_entry['all_emotions']\n",
    "        gaze_direction = emotion_entry['gaze_direction']\n",
    "\n",
    "        # Original emotion-based detection\n",
    "        if (emotion == 'sad' and confidence > 0.40) or (emotion == 'fear' and confidence > 0.4):\n",
    "            self._log_critical_moment('high_stress', emotion_entry, f\"Candidate showed strong signs of {emotion}.\")\n",
    "\n",
    "        if emotion == 'neutral':\n",
    "            self.neutral_streak += 1\n",
    "            if self.neutral_streak == 30:\n",
    "                self._log_critical_moment('prolonged_neutrality', emotion_entry, \"Candidate showed a prolonged neutral expression.\")\n",
    "        else:\n",
    "            self.neutral_streak = 0\n",
    "\n",
    "        # Original low confidence detection\n",
    "        is_ambiguous = (all_emotions.get('neutral', 0) > 0.2 and (all_emotions.get('sad', 0) + all_emotions.get('fear', 0)) > 0.25)\n",
    "        if confidence < 0.45 and is_ambiguous:\n",
    "            self.low_confidence_streak += 1\n",
    "            if self.low_confidence_streak == 15:\n",
    "                self._log_critical_moment('low_confidence', emotion_entry, \"Candidate appeared uncertain.\")\n",
    "        else:\n",
    "            self.low_confidence_streak = 0\n",
    "\n",
    "        # Gaze-based critical moments\n",
    "        if gaze_direction in ['left', 'right'] and emotion_entry['sustained_gaze_low_confidence']:\n",
    "            self._log_critical_moment('gaze_avoidance', emotion_entry, f\"Candidate avoided eye contact by looking {gaze_direction}.\")\n",
    "\n",
    "    def _log_critical_moment(self, type, entry, description):\n",
    "        \"\"\"Helper to append a critical moment to the session log, avoiding rapid duplicates.\"\"\"\n",
    "        if self.current_session['critical_moments']:\n",
    "            last_moment = self.current_session['critical_moments'][-1]\n",
    "            if last_moment['type'] == type and (entry['elapsed_seconds'] - last_moment['elapsed_seconds']) < 5:\n",
    "                return\n",
    "        self.current_session['critical_moments'].append({\n",
    "            'type': type, 'timestamp': entry['timestamp'], 'elapsed_seconds': entry['elapsed_seconds'],\n",
    "            'emotion': entry['dominant_emotion'], 'confidence': entry['confidence'], \n",
    "            'gaze_direction': entry.get('gaze_direction', 'unknown'),\n",
    "            'description': description\n",
    "        })\n",
    "\n",
    "    def generate_comprehensive_summary(self):\n",
    "        \"\"\"Enhanced summary generation including gaze analysis.\"\"\"\n",
    "        if not self.emotion_log:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.emotion_log)\n",
    "        \n",
    "        # Original emotion analysis\n",
    "        emotion_percentages = (df['dominant_emotion'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        emotion_changes = (df['dominant_emotion'].shift() != df['dominant_emotion']).sum()\n",
    "        stability_score = max(0, 100 - (emotion_changes / len(df) * 100)) if len(df) > 0 else 100\n",
    "        \n",
    "        # Gaze analysis\n",
    "        gaze_percentages = (df['gaze_direction'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        gaze_confidence_percentages = (df['gaze_confidence_level'].value_counts(normalize=True) * 100).round(2).to_dict()\n",
    "        engagement_score = gaze_percentages.get('center', 0)\n",
    "        \n",
    "        # Calculate gaze-based low confidence duration\n",
    "        gaze_low_confidence_frames = df['sustained_gaze_low_confidence'].sum()\n",
    "        fps_estimate = 30\n",
    "        gaze_low_confidence_duration = round(gaze_low_confidence_frames / fps_estimate, 2)\n",
    "        \n",
    "        critical_moment_counts = defaultdict(int)\n",
    "        for moment in self.current_session['critical_moments']:\n",
    "            critical_moment_counts[moment['type']] += 1\n",
    "\n",
    "        # Original emotion summary\n",
    "        self.current_session['emotion_summary'] = {\n",
    "            'emotion_percentages': emotion_percentages,\n",
    "            'emotional_stability_score': round(stability_score, 2),\n",
    "            'dominant_emotion_overall': max(emotion_percentages, key=emotion_percentages.get, default='neutral'),\n",
    "            'critical_moment_counts': dict(critical_moment_counts)\n",
    "        }\n",
    "        \n",
    "        # Gaze analysis summary\n",
    "        self.current_session['gaze_analysis'] = {\n",
    "            'gaze_percentages': gaze_percentages,\n",
    "            'gaze_confidence_percentages': gaze_confidence_percentages,\n",
    "            'engagement_score': round(engagement_score, 1),\n",
    "            'looking_at_camera_percentage': gaze_percentages.get('center', 0),\n",
    "            'gaze_low_confidence_duration_seconds': gaze_low_confidence_duration\n",
    "        }\n",
    "\n",
    "    def get_final_report(self):\n",
    "        \"\"\"Enhanced final report including gaze analysis.\"\"\"\n",
    "        if not self.current_session.get('emotion_summary'):\n",
    "            return {\"error\": \"No summary available.\"}\n",
    "        \n",
    "        summary = self.current_session['emotion_summary']\n",
    "        gaze_summary = self.current_session.get('gaze_analysis', {})\n",
    "        \n",
    "        stability = summary['emotional_stability_score']\n",
    "        stress_total = sum(summary.get('critical_moment_counts', {}).values())\n",
    "        engagement = gaze_summary.get('engagement_score', 0)\n",
    "\n",
    "        # Enhanced assessment including gaze\n",
    "        if stability >= 75 and stress_total <= 2 and engagement >= 70:\n",
    "            assessment = \"Excellent\"\n",
    "        elif stability >= 55 and stress_total <= 5 and engagement >= 50:\n",
    "            assessment = \"Good\"\n",
    "        elif stability >= 35 and engagement >= 30:\n",
    "            assessment = \"Fair\"\n",
    "        else:\n",
    "            assessment = \"Needs Attention\"\n",
    "            \n",
    "        return {\n",
    "            'assessment': assessment,\n",
    "            'stability_score': f\"{summary['emotional_stability_score']:.1f}%\",\n",
    "            'dominant_emotion': summary['dominant_emotion_overall'].title(),\n",
    "            'emotion_breakdown_percentage': summary['emotion_percentages'],\n",
    "            'engagement_score': f\"{gaze_summary.get('engagement_score', 0):.1f}%\",\n",
    "            'looking_at_camera_percentage': f\"{gaze_summary.get('looking_at_camera_percentage', 0):.1f}%\",\n",
    "            'gaze_distribution': gaze_summary.get('gaze_percentages', {}),\n",
    "            'gaze_confidence_breakdown': gaze_summary.get('gaze_confidence_percentages', {}),\n",
    "            'gaze_low_confidence_duration': f\"{gaze_summary.get('gaze_low_confidence_duration_seconds', 0):.1f}s\",\n",
    "            'critical_moments_summary': dict(summary.get('critical_moment_counts', {}))\n",
    "        }\n",
    "\n",
    "    def end_interview(self):\n",
    "        \"\"\"Finalizes the interview session and generates comprehensive summary.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            return {\"error\": \"No interview session was started.\"}\n",
    "            \n",
    "        self.interview_active = False\n",
    "        end_time = datetime.datetime.now()\n",
    "        self.current_session['end_time'] = end_time.isoformat()\n",
    "        self.current_session['total_duration'] = round((end_time - self.start_time).total_seconds(), 2)\n",
    "        \n",
    "        self.generate_comprehensive_summary()\n",
    "        \n",
    "        # Save report\n",
    "        report = self.get_final_report()\n",
    "        session_id = self.current_session['session_id']\n",
    "        report_filename = f\"websocket_interview_report_{session_id}.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(report_filename, 'w') as f:\n",
    "                json.dump(report, f, indent=4)\n",
    "            print(f\"📊 Interview Analysis Complete! Report saved: {report_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save report: {e}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "    # WebSocket-specific methods\n",
    "    def process_frame_from_frontend(self, frame_base64):\n",
    "        \"\"\"Process frame received from React frontend\"\"\"\n",
    "        try:\n",
    "            # Decode base64 frame\n",
    "            img_data = base64.b64decode(frame_base64)\n",
    "            np_array = np.frombuffer(img_data, np.uint8)\n",
    "            frame = cv2.imdecode(np_array, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            if not self.interview_active:\n",
    "                # Just return the original frame if interview not started\n",
    "                return self._frame_to_base64(frame), None\n",
    "            \n",
    "            # Your existing emotion analysis\n",
    "            emotion_data = self.analyze_frame(frame)\n",
    "            \n",
    "            # Draw analysis overlay on frame (optional)\n",
    "            annotated_frame = self._draw_analysis_overlay(frame, emotion_data)\n",
    "            \n",
    "            # Convert back to base64\n",
    "            processed_frame_base64 = self._frame_to_base64(annotated_frame)\n",
    "            \n",
    "            return processed_frame_base64, emotion_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def _draw_analysis_overlay(self, frame, emotion_data):\n",
    "        \"\"\"Optional: Draw analysis overlay on frame\"\"\"\n",
    "        if emotion_data:\n",
    "            # Draw face box\n",
    "            face_box = emotion_data['face_box']\n",
    "            x, y, w, h = face_box\n",
    "            \n",
    "            # Draw rectangle around face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "            \n",
    "            # Add emotion and gaze info\n",
    "            emotion = emotion_data['dominant_emotion']\n",
    "            gaze = emotion_data['gaze_direction']\n",
    "            confidence = emotion_data['confidence']\n",
    "            \n",
    "            # Draw text background\n",
    "            cv2.rectangle(frame, (x, y - 80), (x + 300, y), (0, 0, 0), -1)\n",
    "            \n",
    "            # Draw text\n",
    "            cv2.putText(frame, f\"Emotion: {emotion.upper()}\", (x, y - 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Gaze: {gaze.upper()}\", (x, y - 40), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, f\"Conf: {confidence:.2f}\", (x, y - 20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "    def _frame_to_base64(self, frame):\n",
    "        \"\"\"Convert frame to base64\"\"\"\n",
    "        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 80])\n",
    "        return base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "# WebSocket handler\n",
    "analyzer = WebSocketInterviewAnalyzer()\n",
    "\n",
    "async def handle_client(websocket, path):\n",
    "    \"\"\"Handle WebSocket connections from React frontend\"\"\"\n",
    "    print(\"Client connected\")\n",
    "    analyzer.clients.add(websocket)\n",
    "    \n",
    "    try:\n",
    "        async for message in websocket:\n",
    "            data = json.loads(message)\n",
    "            \n",
    "            if data['type'] == 'start_interview':\n",
    "                analyzer.start_interview()\n",
    "                await websocket.send(json.dumps({\n",
    "                    'type': 'interview_started',\n",
    "                    'message': 'Interview analysis started'\n",
    "                }))\n",
    "                \n",
    "            elif data['type'] == 'process_frame':\n",
    "                processed_frame, emotion_data = analyzer.process_frame_from_frontend(data['frame'])\n",
    "                \n",
    "                if processed_frame:\n",
    "                    response = {\n",
    "                        'type': 'processed_frame',\n",
    "                        'frame': processed_frame,\n",
    "                        'emotion_data': emotion_data,\n",
    "                        'interview_active': analyzer.interview_active\n",
    "                    }\n",
    "                    await websocket.send(json.dumps(response))\n",
    "            \n",
    "            elif data['type'] == 'end_interview':\n",
    "                report = analyzer.end_interview()\n",
    "                await websocket.send(json.dumps({\n",
    "                    'type': 'interview_ended',\n",
    "                    'report': report\n",
    "                }))\n",
    "                \n",
    "    except websockets.exceptions.ConnectionClosed:\n",
    "        print(\"Client disconnected\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in WebSocket handler: {e}\")\n",
    "    finally:\n",
    "        if websocket in analyzer.clients:\n",
    "            analyzer.clients.remove(websocket)\n",
    "\n",
    "# Start server\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting WebSocket Interview Analysis Server on ws://localhost:8765\")\n",
    "    start_server = websockets.serve(handle_client, \"localhost\", 8765)\n",
    "    asyncio.get_event_loop().run_until_complete(start_server)\n",
    "    asyncio.get_event_loop().run_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
