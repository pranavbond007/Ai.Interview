<!DOCTYPE html>
<html>
<head>
    <title>Enhanced Emotion Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1/dist/face-api.js"></script>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; margin: 20px; }
        video { border: 2px solid #333; }
        #emotions { margin: 15px; font-size: 16px; }
        .emotion-score { 
            display: inline-block; 
            margin: 5px; 
            padding: 5px 10px; 
            background: #f0f0f0; 
            border-radius: 5px; 
        }
        .dominant { background: #4CAF50; color: white; }
        .sad-emotion { background: #FF6B6B; color: white; }
    </style>
</head>
<body>
    <h1>ðŸŽ­ Enhanced Emotion Detection</h1>
    <div style="position: relative; display: inline-block;">
        <video id="video" width="640" height="480" autoplay muted></video>
        <canvas id="canvas" width="640" height="480" style="position: absolute; top: 0; left: 0;"></canvas>
    </div>
    <div id="status">Loading AI models...</div>
    <div id="emotions"></div>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusDiv = document.getElementById('status');
        const emotionsDiv = document.getElementById('emotions');

        // Enhanced emotion detection with custom thresholds
        function analyzeEmotions(expressions) {
            // Custom thresholds for better sad detection
            const thresholds = {
                happy: 0.3,
                sad: 0.15,      // Lower threshold for sad
                angry: 0.2,
                fearful: 0.2,
                disgusted: 0.2,
                surprised: 0.25,
                neutral: 0.4
            };

            // Create emotion scores with enhanced sad detection
            const emotionScores = {};
            for (const [emotion, value] of Object.entries(expressions)) {
                emotionScores[emotion] = value;
            }

            // Special logic for detecting sadness
            const sadIndicators = {
                lowHappy: expressions.happy < 0.1,
                lowSurprised: expressions.surprised < 0.1, 
                moderateSad: expressions.sad > 0.05,
                neutralNotDominant: expressions.neutral < 0.7
            };

            // If we detect sad indicators, boost sad score
            if (sadIndicators.lowHappy && sadIndicators.moderateSad && sadIndicators.neutralNotDominant) {
                emotionScores.sad = Math.min(1.0, expressions.sad * 2.5);
            }

            // Find emotion that meets threshold requirements
            let detectedEmotion = 'neutral';
            let maxValue = expressions.neutral;

            for (const [emotion, score] of Object.entries(emotionScores)) {
                if (score > thresholds[emotion] && score > maxValue) {
                    detectedEmotion = emotion;
                    maxValue = score;
                }
            }

            return { emotion: detectedEmotion, confidence: maxValue, allScores: emotionScores };
        }

        async function startEmotionDetection() {
            try {
                statusDiv.textContent = "Loading models...";
                
                await faceapi.nets.tinyFaceDetector.load('https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1/model');
                await faceapi.nets.faceExpressionNet.load('https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1/model');
                
                statusDiv.textContent = "Starting camera...";
                
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                
                video.addEventListener('play', () => {
                    statusDiv.textContent = "âœ… Enhanced emotion detection active!";
                    detectEmotions();
                });
                
            } catch (error) {
                statusDiv.textContent = `âŒ Error: ${error.message}`;
                console.error(error);
            }
        }

        let emotionHistory = [];

        async function detectEmotions() {
            try {
                const detection = await faceapi
                    .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceExpressions();

                ctx.clearRect(0, 0, canvas.width, canvas.height);

                if (detection) {
                    const box = detection.detection.box;
                    const expressions = detection.expressions;
                    
                    // Use enhanced emotion analysis
                    const result = analyzeEmotions(expressions);
                    
                    // Add to history for smoothing
                    emotionHistory.push(result.emotion);
                    if (emotionHistory.length > 5) {
                        emotionHistory.shift();
                    }

                    // Get most common emotion from recent history
                    const emotionCounts = {};
                    emotionHistory.forEach(e => emotionCounts[e] = (emotionCounts[e] || 0) + 1);
                    const stableEmotion = Object.keys(emotionCounts).reduce((a, b) => 
                        emotionCounts[a] > emotionCounts[b] ? a : b);

                    // Draw bounding box with color based on emotion
                    const colors = {
                        happy: '#00ff00',
                        sad: '#ff6b6b', 
                        angry: '#ff4444',
                        neutral: '#ffffff',
                        surprised: '#ffff00',
                        fearful: '#800080',
                        disgusted: '#8B4513'
                    };
                    
                    ctx.strokeStyle = colors[stableEmotion] || '#ffffff';
                    ctx.lineWidth = 3;
                    ctx.strokeRect(box.x, box.y, box.width, box.height);

                    // Display emotion with confidence
                    ctx.fillStyle = colors[stableEmotion] || '#ffffff';
                    ctx.font = 'bold 24px Arial';
                    ctx.fillText(`${stableEmotion.toUpperCase()}: ${Math.round(result.confidence * 100)}%`, 
                               box.x, box.y - 10);

                    // Update emotion display
                    let emotionHTML = `<h3>Current Emotion: <span class="emotion-score ${stableEmotion === 'sad' ? 'sad-emotion' : 'dominant'}">${stableEmotion.toUpperCase()} ${Math.round(result.confidence * 100)}%</span></h3>`;
                    
                    emotionHTML += '<p>All Emotions (Raw Scores):</p>';
                    for (const [emotion, value] of Object.entries(result.allScores)) {
                        const percentage = Math.round(value * 100);
                        const className = emotion === stableEmotion ? 'emotion-score dominant' : 'emotion-score';
                        emotionHTML += `<span class="${className}">${emotion}: ${percentage}%</span>`;
                    }
                    
                    // Show detection tips
                    if (stableEmotion === 'sad') {
                        emotionHTML += '<p style="color: #ff6b6b;">ðŸ˜¢ Sadness detected!</p>';
                    } else if (result.allScores.sad > 0.05) {
                        emotionHTML += `<p style="color: #888;">ðŸ’¡ Hint: Sad detected at ${Math.round(result.allScores.sad * 100)}% - try looking more downcast</p>`;
                    }
                    
                    emotionsDiv.innerHTML = emotionHTML;
                } else {
                    emotionsDiv.innerHTML = "<p>No face detected</p>";
                }

            } catch (error) {
                console.error("Detection error:", error);
            }

            requestAnimationFrame(detectEmotions);
        }

        startEmotionDetection();
    </script>
</body>
</html>
