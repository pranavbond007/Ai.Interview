{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd909f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e96790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted job: DY4q9p5tzt4od9tr\n",
      "Waiting for transcription...\n",
      "Waiting for transcription...\n",
      "Waiting for transcription...\n",
      "<silence> Hello, my name is Ani and uh, as you know, I'm trying to build a project. You know, I'm facing very much difficulties right now because, uh, in my mind there is a lot of things happening right now. Uh, but I think so I can welcome that because God is in my side. I know I'm just making this audio to try and test my <inaudible> that is it able to detect any pauses between the audios and show it, show it in the, you know, text. Uh, that's why I don't use this words like, uh, but I'm using these words so that my model can become better. Thank you.\n"
     ]
    }
   ],
   "source": [
    "from rev_ai import apiclient\n",
    "import os\n",
    "\n",
    "ACCESS_TOKEN = \"022noQShzOFkDaBPRnGKcWRfLg_Caq76eOW5FodAT_nyILtUKdPZAJ7_G97GXvmUGTPTpDnyTXIgU0SBZXngPXpA94pW4\"\n",
    "AUDIO_FILE = \"New Recording 49.m4a\"\n",
    "\n",
    "client = apiclient.RevAiAPIClient(ACCESS_TOKEN)\n",
    "\n",
    "# 1. Submit file for transcription\n",
    "job = client.submit_job_local_file(AUDIO_FILE)\n",
    "\n",
    "print(f\"Submitted job: {job.id}\")\n",
    "\n",
    "# 2. Wait until done\n",
    "job_details = client.get_job_details(job.id)\n",
    "while job_details.status not in [\"transcribed\", \"failed\"]:\n",
    "    import time\n",
    "    print(\"Waiting for transcription...\")\n",
    "    time.sleep(5)\n",
    "    job_details = client.get_job_details(job.id)\n",
    "\n",
    "# 3. Get transcript object\n",
    "transcript = client.get_transcript_object(job.id)\n",
    "output_text = \"\"\n",
    "last_end = 0.0\n",
    "\n",
    "for monologue in transcript.monologues:\n",
    "    for element in monologue.elements:\n",
    "        # Some elements may not have timestamps (like punctuation)\n",
    "        if hasattr(element, \"ts\") and hasattr(element, \"end_ts\"):\n",
    "            start = element.ts\n",
    "            if start and last_end and (start - last_end) >= 3.0:\n",
    "                pause_len = round(start - last_end)\n",
    "                output_text += f\" [PAUSE:{pause_len}s] \"\n",
    "            output_text += element.value + \" \"\n",
    "            last_end = element.end_ts\n",
    "        else:\n",
    "            # punctuation or other tokens without timestamps\n",
    "            output_text += element.value\n",
    "\n",
    "print(output_text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ccc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849013e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2fd9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted job: Cfp5iXrhTHLNOooM\n",
      "Waiting for transcription...\n",
      "Waiting for transcription...\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE115400>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FDFC2D50>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FDFC3750>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE023BB0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE023230>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE08AF90>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE08DBF0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE08E030>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE06B550>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE06B750>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0A6D50>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0A6B70>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE07E970>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE07E350>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FDE1DBF0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FDFD6450>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FDFD65D0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE027E30>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE027CD0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FDD4D810>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE110A50>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE09A7B0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE11F7D0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE11F850>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CC980>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCAD0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCC20>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCD00>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCC90>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCD70>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCDE0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCE50>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCEC0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCF30>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CCFA0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD010>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD080>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD0F0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD160>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD1D0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD240>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD2B0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD320>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD390>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD400>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD470>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD4E0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD550>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD5C0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD630>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD6A0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD710>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD780>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD7F0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD860>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD8D0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD940>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CD9B0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDA20>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDA90>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDB00>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDB70>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDBE0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDC50>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDCC0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDD30>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDDA0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDE10>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDE80>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDEF0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDF60>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CDFD0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE040>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE0B0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE120>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE190>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE200>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE270>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE2E0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE350>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE3C0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE430>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE4A0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE510>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE580>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE5F0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE660>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE6D0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE740>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE7B0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE820>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE890>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE900>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE970>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CE9E0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEA50>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEAC0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEB30>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEBA0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEC10>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEC80>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CECF0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CED60>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEDD0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEE40>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEEB0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEF20>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CEF90>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF000>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF070>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF0E0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF150>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF1C0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF230>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF2A0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF310>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF380>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF3F0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF460>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF4D0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF540>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF5B0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF620>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF690>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF700>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF770>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF7E0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF850>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF8C0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF930>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CF9A0>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CFA10>\n",
      "Element: <rev_ai.models.asynchronous.transcript.Element object at 0x00000190FE0CFA80>\n"
     ]
    }
   ],
   "source": [
    "from rev_ai import apiclient\n",
    "from pydub import AudioSegment, silence\n",
    "import time\n",
    "\n",
    "# ---- 1. Rev AI transcription ----\n",
    "ACCESS_TOKEN = \"022noQShzOFkDaBPRnGKcWRfLg_Caq76eOW5FodAT_nyILtUKdPZAJ7_G97GXvmUGTPTpDnyTXIgU0SBZXngPXpA94pW4\"   # put your Rev AI token here\n",
    "AUDIO_FILE = \"New Recording 48.m4a\"   # your M4A file\n",
    "\n",
    "client = apiclient.RevAiAPIClient(ACCESS_TOKEN)\n",
    "\n",
    "job = client.submit_job_local_file(AUDIO_FILE)\n",
    "print(f\"Submitted job: {job.id}\")\n",
    "\n",
    "# wait until transcription completes\n",
    "job_details = client.get_job_details(job.id)\n",
    "while job_details.status not in [\"transcribed\", \"failed\"]:\n",
    "    print(\"Waiting for transcription...\")\n",
    "    time.sleep(5)\n",
    "    job_details = client.get_job_details(job.id)\n",
    "\n",
    "if job_details.status == \"failed\":\n",
    "    raise RuntimeError(\"Rev AI transcription failed!\")\n",
    "\n",
    "transcript = client.get_transcript_object(job.id)\n",
    "\n",
    "# ---- 2. Detect pauses with PyDub ----\n",
    "# load M4A file (requires ffmpeg installed)\n",
    "audio = AudioSegment.from_file(AUDIO_FILE, format=\"m4a\")\n",
    "\n",
    "silences = silence.detect_silence(\n",
    "    audio,\n",
    "    min_silence_len=3000,  # 3 sec\n",
    "    silence_thresh=-40     # adjust if needed (depends on recording volume)\n",
    ")\n",
    "# silence list = [(start_ms, end_ms), ...]\n",
    "\n",
    "# ---- 3. Merge transcript + pauses ----\n",
    "output_text = \"\"\n",
    "last_end = None\n",
    "\n",
    "for monologue in transcript.monologues:\n",
    "    for element in monologue.elements:\n",
    "         print(\"Element:\", element)\n",
    "         if hasattr(element, \"ts\") and hasattr(element, \"end_ts\"):  \n",
    "            start = element.ts\n",
    "            if last_end is not None:\n",
    "                pause = start - last_end\n",
    "                if pause >= 3.0:  # pause threshold in seconds\n",
    "                    output_text += f\"[PAUSE:{int(round(pause))}s] \"\n",
    "            \n",
    "            # Add the actual word/punctuation\n",
    "            if element.value in [\".\", \",\", \"?\", \"!\", \";\", \":\"]:\n",
    "                output_text = output_text.rstrip() + element.value + \" \"\n",
    "            else:\n",
    "                output_text += element.value + \" \"\n",
    "            \n",
    "            last_end = element.end_ts\n",
    "         else:\n",
    "            # if no timestamps, just add the value\n",
    "            output_text += element.value + \" \"\n",
    "\n",
    "# Cleanup spacing\n",
    "output_text = \" \".join(output_text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b727df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for transcription...\n",
      "Waiting for transcription...\n",
      "Element value: 'Hello'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ','\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'my'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'name'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'is'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'Brenda'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'Ani'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'and'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'I'm'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'from'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'IT'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'Branch'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'and'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'I'm'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'a'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'very'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'heavy'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'person'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: '.'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'My'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'life'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'is'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'very'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'great'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: '.'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'I'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'have'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'many'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'lovely'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'friends'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: '.'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'Uh'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ','\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'the'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'reason'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'is'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'that'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'is'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ','\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'uh'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ','\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'you'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'know'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ','\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'no'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ','\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'but'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'I'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'love'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'my'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'work'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: '.'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'I'm'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'very'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'passionate'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'and'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'currently'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'I'm'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'building'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'up'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'project'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ','\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'I'm'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'building'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'a'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'project'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'called'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'AI'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'Interview'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'in'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'which'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: ' '\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: 'is'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "Element value: '.'\n",
      "Element start time: No timestamp\n",
      "Element end time: No timestamp\n",
      "---\n",
      "\n",
      "==================================================\n",
      "FINAL TRANSCRIPT WITH PAUSES:\n",
      "==================================================\n",
      "Hello, my name is Brenda Ani and I'm from IT Branch and I'm a very heavy person. My life is very great. I have many lovely friends. Uh, the reason is that is, uh, you know, no, but I love my work. I'm very passionate and currently I'm building up project, I'm building a project called AI Interview in which is.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from rev_ai import apiclient\n",
    "from pydub import AudioSegment, silence\n",
    "import time\n",
    "\n",
    "# ---- 1. Rev AI transcription ----\n",
    "ACCESS_TOKEN = \"022noQShzOFkDaBPRnGKcWRfLg_Caq76eOW5FodAT_nyILtUKdPZAJ7_G97GXvmUGTPTpDnyTXIgU0SBZXngPXpA94pW4\"\n",
    "AUDIO_FILE = \"New Recording 48.m4a\"\n",
    "\n",
    "client = apiclient.RevAiAPIClient(ACCESS_TOKEN)\n",
    "\n",
    "job = client.submit_job_local_file(\n",
    "        AUDIO_FILE,\n",
    "        verbatim=True,  # This should include filler words AND timestamps\n",
    "        remove_disfluencies=False\n",
    "    )\n",
    "    \n",
    "# Wait until transcription completes\n",
    "job_details = client.get_job_details(job.id)\n",
    "while job_details.status not in [\"transcribed\", \"failed\"]:\n",
    "    print(\"Waiting for transcription...\")\n",
    "    time.sleep(5)\n",
    "    job_details = client.get_job_details(job.id)\n",
    "\n",
    "if job_details.status == \"failed\":\n",
    "    raise RuntimeError(\"Rev AI transcription failed!\")\n",
    "\n",
    "transcript = client.get_transcript_object(job.id)\n",
    "\n",
    "# ---- 2. Build transcript with pause detection ----\n",
    "output_text = \"\"\n",
    "last_end_time = None\n",
    "\n",
    "for monologue in transcript.monologues:\n",
    "    for element in monologue.elements:\n",
    "        # Debug: Print what we actually have access to\n",
    "        print(f\"Element value: '{element.value}'\")\n",
    "        print(f\"Element start time: {getattr(element, 'ts', 'No timestamp')}\")\n",
    "        print(f\"Element end time: {getattr(element, 'end_ts', 'No timestamp')}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "        # Check if element has timestamps\n",
    "        if hasattr(element, 'ts') and element.ts is not None:\n",
    "            start_time = element.ts\n",
    "            \n",
    "            # Check for pause since last element\n",
    "            if last_end_time is not None:\n",
    "                pause_duration = start_time - last_end_time\n",
    "                if pause_duration >= 3.0:  # 3 second threshold\n",
    "                    output_text += f\" [PAUSE:{pause_duration:.1f}s] \"\n",
    "            \n",
    "            # Update last end time\n",
    "            if hasattr(element, 'end_ts') and element.end_ts is not None:\n",
    "                last_end_time = element.end_ts\n",
    "            else:\n",
    "                last_end_time = start_time  # fallback to start time\n",
    "        \n",
    "        # Add the element value (word/punctuation)\n",
    "        if element.value and element.value.strip():  # Only add non-empty values\n",
    "            if element.value in [\".\", \",\", \"?\", \"!\", \";\", \":\"]:\n",
    "                # Remove space before punctuation\n",
    "                output_text = output_text.rstrip() + element.value + \" \"\n",
    "            else:\n",
    "                output_text += element.value + \" \"\n",
    "\n",
    "# Clean up extra spaces\n",
    "output_text = \" \".join(output_text.split())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TRANSCRIPT WITH PAUSES:\")\n",
    "print(\"=\"*50)\n",
    "print(output_text)\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bc9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 401 Client Error: Unauthorized for url: https://api.rev.ai/speechtotext/v1/jobs; Server Response : {\"title\":\"Authorization has been denied for this request.\",\"status\":401}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b5f644",
   "metadata": {},
   "source": [
    "## Crisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f363fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted M4A to WAV successfully!\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Convert M4A to WAV\n",
    "audio = AudioSegment.from_file(\"WhatsApp Audio 2025-09-02 at 7.12.44 PM.mp4\", format=\"mp4\")\n",
    "audio.export(\"WhatsApp Audio 2025-09-02 at 7.12.44 PM.wav\", format=\"wav\")\n",
    "\n",
    "print(\"Converted M4A to WAV successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ff3a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CrisperWhisper model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing: New Recording 48.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 1\n",
      "Make sure you have installed the required packages and logged into Hugging Face\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "def transcribe_with_crisper_whisper(audio_file, pause_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Use CrisperWhisper for verbatim transcription with filler words and timestamps\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    model_id = \"nyrahealth/CrisperWhisper\"\n",
    "    \n",
    "    print(\"Loading CrisperWhisper model...\")\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, \n",
    "        torch_dtype=torch_dtype, \n",
    "        low_cpu_mem_usage=True, \n",
    "        use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    # Create pipeline with word-level timestamps\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,\n",
    "        return_timestamps='word',  # This ensures word-level timestamps\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    print(f\"Transcribing: {audio_file}\")\n",
    "    result = pipe(audio_file)\n",
    "    \n",
    "    # Process chunks to add pause markers\n",
    "    chunks = result.get(\"chunks\", [])\n",
    "    final_transcript = \"\"\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        final_transcript += chunk[\"text\"]\n",
    "        \n",
    "        # Check for pauses between chunks\n",
    "        if i < len(chunks) - 1:\n",
    "            current_end = chunk.get(\"timestamp\", [None, None])[1]\n",
    "            next_start = chunks[i + 1].get(\"timestamp\", [None, None])[0]\n",
    "            \n",
    "            if current_end and next_start:\n",
    "                pause_duration = next_start - current_end\n",
    "                if pause_duration >= pause_threshold:\n",
    "                    final_transcript += f\" [PAUSE:{pause_duration:.1f}s]\"\n",
    "    \n",
    "    return {\n",
    "        \"transcript\": final_transcript,\n",
    "        \"chunks_with_timestamps\": chunks,\n",
    "        \"raw_result\": result\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    # You need to install: pip install transformers torch datasets accelerate\n",
    "    # And login: huggingface-cli login\n",
    "    \n",
    "    result = transcribe_with_crisper_whisper(\"New Recording 48.wav\", pause_threshold=3.0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CRISPERWHISPER TRANSCRIPT WITH FILLER WORDS AND PAUSES:\")\n",
    "    print(\"=\"*60)\n",
    "    print(result[\"transcript\"])\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Print word-level timestamps\n",
    "    print(\"\\nWORD-LEVEL TIMESTAMPS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for chunk in result[\"chunks_with_timestamps\"]:\n",
    "        if chunk.get(\"timestamp\"):\n",
    "            start, end = chunk[\"timestamp\"]\n",
    "            print(f\"{chunk['text']} | {start:.2f}s - {end:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure you have installed the required packages and logged into Hugging Face\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Faster CrisperWhisper on cuda...\n",
      "Error with Faster CrisperWhisper: CUDA failed with error out of memory\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "\n",
    "def transcribe_with_faster_crisper(audio_file, pause_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Use faster-whisper version of CrisperWhisper - most stable option\n",
    "    \"\"\"\n",
    "    model_name = 'nyrahealth/faster_CrisperWhisper'\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    compute_type = \"float16\" if torch.cuda.is_available() else \"float32\"\n",
    "    \n",
    "    print(f\"Loading Faster CrisperWhisper on {device}...\")\n",
    "    model = WhisperModel(model_name, device=device, compute_type=compute_type)\n",
    "    \n",
    "    print(f\"Transcribing: {audio_file}\")\n",
    "    segments, info = model.transcribe(\n",
    "        audio_file, \n",
    "        beam_size=1, \n",
    "        language='en', \n",
    "        word_timestamps=True,\n",
    "        without_timestamps=False,\n",
    "        vad_filter=True,  # Voice activity detection\n",
    "        vad_parameters=dict(min_silence_duration_ms=500)\n",
    "    )\n",
    "    \n",
    "    # Process segments to build transcript with pauses\n",
    "    transcript = \"\"\n",
    "    word_timestamps = []\n",
    "    last_end_time = None\n",
    "    \n",
    "    for segment in segments:\n",
    "        # Check for pause between segments\n",
    "        if last_end_time is not None:\n",
    "            pause_duration = segment.start - last_end_time\n",
    "            if pause_duration >= pause_threshold:\n",
    "                transcript += f\" [PAUSE:{pause_duration:.1f}s] \"\n",
    "        \n",
    "        # Add segment text (includes filler words)\n",
    "        transcript += segment.text\n",
    "        last_end_time = segment.end\n",
    "        \n",
    "        # Collect word-level timestamps if available\n",
    "        if hasattr(segment, 'words') and segment.words:\n",
    "            for word in segment.words:\n",
    "                word_timestamps.append({\n",
    "                    \"word\": word.word,\n",
    "                    \"start\": word.start,\n",
    "                    \"end\": word.end\n",
    "                })\n",
    "        \n",
    "        # Print segment for debugging\n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "    \n",
    "    return {\n",
    "        \"transcript\": transcript.strip(),\n",
    "        \"word_timestamps\": word_timestamps,\n",
    "        \"segments\": list(segments)  # Convert generator to list\n",
    "    }\n",
    "\n",
    "# Install faster-whisper first\n",
    "# pip install faster-whisper\n",
    "\n",
    "try:\n",
    "    result = transcribe_with_faster_crisper(\"New Recording 48.wav\", pause_threshold=3.0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FASTER CRISPERWHISPER TRANSCRIPT WITH FILLER WORDS AND PAUSES:\")\n",
    "    print(\"=\"*60)\n",
    "    print(result[\"transcript\"])\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if result[\"word_timestamps\"]:\n",
    "        print(\"\\nWORD-LEVEL TIMESTAMPS:\")\n",
    "        print(\"-\" * 40)\n",
    "        for word_info in result[\"word_timestamps\"][:20]:  # Show first 20 words\n",
    "            print(f\"'{word_info['word']}' | {word_info['start']:.2f}s - {word_info['end']:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with Faster CrisperWhisper: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8b81ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CrisperWhisper on CPU...\n",
      "Transcribing: New Recording 48.wav\n",
      "DEBUG: Found 0 segments\n",
      "DEBUG: No segments found! Trying without VAD filter...\n",
      "DEBUG: After disabling VAD: Found 2 segments\n",
      "DEBUG: Processing segment 1: 'Hello,mynameisbrandabhani,andiamfromitbranch,Niti,Shibinaur.Iamaveryhappyperson,mylifisverygreat,ihavemanylovelyfriends,andiloveyou.'\n",
      "[0.00s -> 15.58s] Hello,mynameisbrandabhani,andiamfromitbranch,Niti,Shibinaur.Iamaveryhappyperson,mylifisverygreat,ihavemanylovelyfriends,andiloveyou.\n",
      "DEBUG: Processing segment 2: 'No,but,I,Ilovemywork,Iamverypassionate,and,currently,Iambuildingaproject,Iambuildingaproject,called,A.I.Interview,in,which.'\n",
      "[30.40s -> 42.24s] No,but,I,Ilovemywork,Iamverypassionate,and,currently,Iambuildingaproject,Iambuildingaproject,called,A.I.Interview,in,which.\n",
      "\n",
      "Final Transcript:\n",
      "==================================================\n",
      "Hello,mynameisbrandabhani,andiamfromitbranch,Niti,Shibinaur.Iamaveryhappyperson,mylifisverygreat,ihavemanylovelyfriends,andiloveyou. [PAUSE:14.8s] No,but,I,Ilovemywork,Iamverypassionate,and,currently,Iambuildingaproject,Iambuildingaproject,called,A.I.Interview,in,which.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "\n",
    "def transcribe_with_cpu_crisper_debug(audio_file, pause_threshold=3.0):\n",
    "    \"\"\"\n",
    "    Debug version to see what's happening\n",
    "    \"\"\"\n",
    "    model_name = 'nyrahealth/faster_CrisperWhisper'\n",
    "    \n",
    "    print(\"Loading CrisperWhisper on CPU...\")\n",
    "    model = WhisperModel(\n",
    "        model_name, \n",
    "        device=\"cpu\",\n",
    "        compute_type=\"float32\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Transcribing: {audio_file}\")\n",
    "    segments, info = model.transcribe(\n",
    "        audio_file, \n",
    "        beam_size=1, \n",
    "        language='en', \n",
    "        word_timestamps=True,\n",
    "        vad_filter=True\n",
    "    )\n",
    "    \n",
    "    # Debug: Check if we got any segments\n",
    "    segments_list = list(segments)  # Convert generator to list\n",
    "    print(f\"DEBUG: Found {len(segments_list)} segments\")\n",
    "    \n",
    "    if len(segments_list) == 0:\n",
    "        print(\"DEBUG: No segments found! Trying without VAD filter...\")\n",
    "        segments, info = model.transcribe(\n",
    "            audio_file, \n",
    "            beam_size=1, \n",
    "            language='en', \n",
    "            word_timestamps=True,\n",
    "            vad_filter=False  # Disable voice activity detection\n",
    "        )\n",
    "        segments_list = list(segments)\n",
    "        print(f\"DEBUG: After disabling VAD: Found {len(segments_list)} segments\")\n",
    "    \n",
    "    # Process segments\n",
    "    transcript = \"\"\n",
    "    last_end_time = None\n",
    "    \n",
    "    for i, segment in enumerate(segments_list):\n",
    "        print(f\"DEBUG: Processing segment {i+1}: '{segment.text}'\")\n",
    "        \n",
    "        if last_end_time is not None:\n",
    "            pause_duration = segment.start - last_end_time\n",
    "            if pause_duration >= pause_threshold:\n",
    "                transcript += f\" [PAUSE:{pause_duration:.1f}s] \"\n",
    "        \n",
    "        transcript += segment.text\n",
    "        last_end_time = segment.end\n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "# Run debug version\n",
    "result = transcribe_with_cpu_crisper_debug(\"New Recording 48.wav\")\n",
    "print(\"\\nFinal Transcript:\")\n",
    "print(\"=\"*50)\n",
    "print(result)\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d97cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CrisperWhisper on CPU...\n",
      "Transcribing: question_1_1757185865994.wav\n",
      "[0.00s -> 10.70s] Hello Hello Hello Matharchod Hello Hello Yehmaphogos Yeh Gir Gir Gir Howareyouall mynameisprayanchod.\n",
      "\n",
      "Cleaned Final Transcript:\n",
      "============================================================\n",
      "Hello Hello Hello Matharchod Hello Hello Yehmaphogos Yeh Gir Gir Gir Howareyouall mynameisprayanchod.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import re\n",
    "\n",
    "def transcribe_with_cpu_crisper_clean(audio_file, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Run CrisperWhisper on CPU with cleaned output\n",
    "    \"\"\"\n",
    "    model_name = 'nyrahealth/faster_CrisperWhisper'\n",
    "    \n",
    "    print(\"Loading CrisperWhisper on CPU...\")\n",
    "    model = WhisperModel(\n",
    "        model_name, \n",
    "        device=\"cpu\",\n",
    "        compute_type=\"float32\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Transcribing: {audio_file}\")\n",
    "    segments, info = model.transcribe(\n",
    "        audio_file, \n",
    "        beam_size=1, \n",
    "        language='en', \n",
    "        word_timestamps=True,\n",
    "        vad_filter=False  # Keep this disabled since VAD was blocking segments\n",
    "    )\n",
    "    \n",
    "    # Process segments\n",
    "    transcript = \"\"\n",
    "    last_end_time = None\n",
    "    \n",
    "    for segment in list(segments):\n",
    "        if last_end_time is not None:\n",
    "            pause_duration = segment.start - last_end_time\n",
    "            if pause_duration >= pause_threshold:\n",
    "                transcript += f\" [PAUSE:{pause_duration:.1f}s] \"\n",
    "        \n",
    "        # Clean up the segment text\n",
    "        cleaned_text = clean_transcript_text(segment.text)\n",
    "        transcript += cleaned_text\n",
    "        last_end_time = segment.end\n",
    "        \n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {cleaned_text}\")\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "def clean_transcript_text(text):\n",
    "    \"\"\"\n",
    "    Clean up transcript text by removing unwanted commas and fixing spacing\n",
    "    \"\"\"\n",
    "    # Remove commas that separate words (but keep commas that are actual punctuation)\n",
    "    # Replace commas followed by lowercase letters with spaces\n",
    "    text = re.sub(r',([a-z])', r' \\1', text)\n",
    "    \n",
    "    # Replace remaining isolated commas with spaces\n",
    "    text = re.sub(r',\\s*', ' ', text)\n",
    "    \n",
    "    # Fix multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Capitalize first letter of sentences\n",
    "    text = re.sub(r'^\\w', lambda m: m.group(0).upper(), text)\n",
    "    text = re.sub(r'\\.\\s*\\w', lambda m: m.group(0).upper(), text)\n",
    "    \n",
    "    # Add proper punctuation spacing\n",
    "    text = re.sub(r'\\.([A-Z])', r'. \\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Usage\n",
    "result = transcribe_with_cpu_crisper_clean(\"question_1_1757185865994.wav\")\n",
    "print(\"\\nCleaned Final Transcript:\")\n",
    "print(\"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03457a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3dfd8cc",
   "metadata": {},
   "source": [
    "## YEH Pause short pauses vaala code hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e43b1123",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetect_audio_pauses\u001b[39m(audio_file, pause_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, silence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def detect_audio_pauses(audio_file, pause_threshold=2.0, silence_threshold=-40):\n",
    "    \"\"\"\n",
    "    Detect pauses directly from audio waveform\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # Convert to decibels\n",
    "    audio_db = librosa.amplitude_to_db(np.abs(audio), ref=np.max)\n",
    "    \n",
    "    # Detect silence (below threshold)\n",
    "    silence_mask = audio_db < silence_threshold\n",
    "    \n",
    "    # Find continuous silence regions\n",
    "    silence_regions = []\n",
    "    in_silence = False\n",
    "    silence_start = 0\n",
    "    \n",
    "    for i, is_silent in enumerate(silence_mask):\n",
    "        if is_silent and not in_silence:\n",
    "            silence_start = i\n",
    "            in_silence = True\n",
    "        elif not is_silent and in_silence:\n",
    "            silence_duration = (i - silence_start) / sr\n",
    "            if silence_duration >= pause_threshold:\n",
    "                silence_regions.append({\n",
    "                    'start': silence_start / sr,\n",
    "                    'end': i / sr,\n",
    "                    'duration': silence_duration\n",
    "                })\n",
    "            in_silence = False\n",
    "    \n",
    "    print(f\"Found {len(silence_regions)} pauses >= {pause_threshold}s:\")\n",
    "    for region in silence_regions:\n",
    "        print(f\"  Pause: {region['start']:.2f}s - {region['end']:.2f}s ({region['duration']:.1f}s)\")\n",
    "    \n",
    "    return silence_regions\n",
    "\n",
    "def transcribe_with_audio_pause_detection(audio_file, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Combine CrisperWhisper with audio-based pause detection\n",
    "    \"\"\"\n",
    "    # Get transcription\n",
    "    model = WhisperModel('nyrahealth/faster_CrisperWhisper', device=\"cpu\", compute_type=\"float32\")\n",
    "    segments, info = model.transcribe(audio_file, beam_size=1, language='en', vad_filter=False)\n",
    "    segments_list = list(segments)\n",
    "    \n",
    "    # Get audio-based pauses\n",
    "    audio_pauses = detect_audio_pauses(audio_file, pause_threshold)\n",
    "    \n",
    "    # Build transcript with both segment-based and audio-based pauses\n",
    "    transcript = \"\"\n",
    "    \n",
    "    for i, segment in enumerate(segments_list):\n",
    "        cleaned_text = clean_transcript_text(segment.text)\n",
    "        \n",
    "        # Check for audio-based pauses within this segment\n",
    "        segment_start = segment.start\n",
    "        segment_end = segment.end\n",
    "        \n",
    "        # Add pauses that occur after this segment\n",
    "        for pause in audio_pauses:\n",
    "            if segment_end <= pause['start'] < (segments_list[i+1].start if i+1 < len(segments_list) else float('inf')):\n",
    "                transcript += f\" [PAUSE:{pause['duration']:.1f}s] \"\n",
    "        \n",
    "        transcript += cleaned_text + \" \"\n",
    "        \n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {cleaned_text}\")\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "# Usage - install librosa first: pip install librosa\n",
    "result = transcribe_with_audio_pause_detection(\"New Recording 48.wav\", pause_threshold=2.0)\n",
    "print(\"\\nTranscript with audio-based pause detection:\")\n",
    "print(\"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77c423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c76a03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetect_audio_pauses\u001b[39m(audio_file, pause_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, silence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def detect_audio_pauses(audio_file, pause_threshold=2.0, silence_threshold=-40):\n",
    "    \"\"\"\n",
    "    Detect pauses directly from audio waveform\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # Convert to decibels\n",
    "    audio_db = librosa.amplitude_to_db(np.abs(audio), ref=np.max)\n",
    "    \n",
    "    # Detect silence (below threshold)\n",
    "    silence_mask = audio_db < silence_threshold\n",
    "    \n",
    "    # Find continuous silence regions\n",
    "    silence_regions = []\n",
    "    in_silence = False\n",
    "    silence_start = 0\n",
    "    \n",
    "    for i, is_silent in enumerate(silence_mask):\n",
    "        if is_silent and not in_silence:\n",
    "            silence_start = i\n",
    "            in_silence = True\n",
    "        elif not is_silent and in_silence:\n",
    "            silence_duration = (i - silence_start) / sr\n",
    "            if silence_duration >= pause_threshold:\n",
    "                silence_regions.append({\n",
    "                    'start': silence_start / sr,\n",
    "                    'end': i / sr,\n",
    "                    'duration': silence_duration\n",
    "                })\n",
    "            in_silence = False\n",
    "    \n",
    "    print(f\"Found {len(silence_regions)} pauses >= {pause_threshold}s:\")\n",
    "    for region in silence_regions:\n",
    "        print(f\"  Pause: {region['start']:.2f}s - {region['end']:.2f}s ({region['duration']:.1f}s)\")\n",
    "    \n",
    "    return silence_regions\n",
    "\n",
    "def transcribe_with_audio_pause_detection(audio_file, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Combine CrisperWhisper with audio-based pause detection\n",
    "    \"\"\"\n",
    "    # Get transcription\n",
    "    model = WhisperModel('nyrahealth/faster_CrisperWhisper', device=\"cpu\", compute_type=\"float32\")\n",
    "    segments, info = model.transcribe(audio_file, beam_size=1, language='en', vad_filter=False)\n",
    "    segments_list = list(segments)\n",
    "    \n",
    "    # Get audio-based pauses\n",
    "    audio_pauses = detect_audio_pauses(audio_file, pause_threshold)\n",
    "    \n",
    "    # Build transcript with both segment-based and audio-based pauses\n",
    "    transcript = \"\"\n",
    "    \n",
    "    for i, segment in enumerate(segments_list):\n",
    "        cleaned_text = clean_transcript_text(segment.text)\n",
    "        \n",
    "        # Check for audio-based pauses within this segment\n",
    "        segment_start = segment.start\n",
    "        segment_end = segment.end\n",
    "        \n",
    "        # Add pauses that occur after this segment\n",
    "        for pause in audio_pauses:\n",
    "            if segment_end <= pause['start'] < (segments_list[i+1].start if i+1 < len(segments_list) else float('inf')):\n",
    "                transcript += f\" [PAUSE:{pause['duration']:.1f}s] \"\n",
    "        \n",
    "        transcript += cleaned_text + \" \"\n",
    "        \n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {cleaned_text}\")\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "# Usage - install librosa first: pip install librosa\n",
    "result = transcribe_with_audio_pause_detection(\"New Recording 48.wav\", pause_threshold=2.0)\n",
    "print(\"\\nTranscript with audio-based pause detection:\")\n",
    "print(\"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d11a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR Segments: 3\n",
      "Audio-based silences found: 2\n",
      "  Silence 1: 22.74s - 26.08s (3.3s)\n",
      "  Silence 2: 39.61s - 42.50s (2.9s)\n",
      "\n",
      "Transcript with Audio-Based Pause Detection:\n",
      "============================================================\n",
      "Hello mynameismuthubani and[UH] asyouknow I'mtryingtobuildaproject.Youknow I'mfacingverymuchdifficultiesrightnow because [UH] inmymind there'salotofthingshappeningrightnow [UH]but Ithink so Icanovercomethat because Godisnmyside Iknow I'mjustmakingthisaudio to [UH]tryandtestmy. [UH] I'm. Don't.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "from pydub import AudioSegment, silence\n",
    "import re\n",
    "\n",
    "def transcribe_with_audio_pause_detection(audio_file, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Combine CrisperWhisper transcription with PyDub silence detection\n",
    "    \"\"\"\n",
    "    # Get transcription from CrisperWhisper\n",
    "    model = WhisperModel('nyrahealth/faster_CrisperWhisper', device=\"cpu\", compute_type=\"float32\")\n",
    "    segments, info = model.transcribe(\n",
    "        audio_file, \n",
    "        beam_size=1, \n",
    "        language='en', \n",
    "        word_timestamps=True,\n",
    "        vad_filter=False\n",
    "    )\n",
    "    \n",
    "    segments_list = list(segments)\n",
    "    print(f\"ASR Segments: {len(segments_list)}\")\n",
    "    \n",
    "    # Load audio and detect actual silences\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    \n",
    "    # Detect silences >= pause_threshold seconds\n",
    "    silence_thresh = -40  # dB - adjust if needed\n",
    "    silences = silence.detect_silence(\n",
    "        audio,\n",
    "        min_silence_len=int(pause_threshold * 1000),  # Convert to milliseconds\n",
    "        silence_thresh=silence_thresh\n",
    "    )\n",
    "    \n",
    "    print(f\"Audio-based silences found: {len(silences)}\")\n",
    "    for i, (start_ms, end_ms) in enumerate(silences):\n",
    "        duration = (end_ms - start_ms) / 1000\n",
    "        print(f\"  Silence {i+1}: {start_ms/1000:.2f}s - {end_ms/1000:.2f}s ({duration:.1f}s)\")\n",
    "    \n",
    "    # Build transcript by inserting pauses from audio analysis\n",
    "    transcript = \"\"\n",
    "    \n",
    "    for i, segment in enumerate(segments_list):\n",
    "        # Clean segment text\n",
    "        cleaned_text = clean_transcript_text(segment.text)\n",
    "        transcript += cleaned_text\n",
    "        \n",
    "        # Check if there's a silence after this segment ends\n",
    "        segment_end_s = segment.end\n",
    "        \n",
    "        # Find silences that start near this segment's end\n",
    "        for silence_start_ms, silence_end_ms in silences:\n",
    "            silence_start_s = silence_start_ms / 1000\n",
    "            silence_duration = (silence_end_ms - silence_start_ms) / 1000\n",
    "            \n",
    "            # If silence starts within 1 second of segment end, it's likely related\n",
    "            if abs(silence_start_s - segment_end_s) <= 1.0:\n",
    "                transcript += f\" [PAUSE:{silence_duration:.1f}s] \"\n",
    "                print(f\"Added pause after '{cleaned_text}': {silence_duration:.1f}s\")\n",
    "                break\n",
    "        \n",
    "        transcript += \" \"\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "def clean_transcript_text(text):\n",
    "    \"\"\"Clean transcript text\"\"\"\n",
    "    text = re.sub(r',([a-z])', r' \\1', text)\n",
    "    text = re.sub(r',\\s*', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    if text:\n",
    "        text = text[0].upper() + text[1:]\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "result = transcribe_with_audio_pause_detection(\"New Recording 49.wav\", pause_threshold=2.0)\n",
    "print(\"\\nTranscript with Audio-Based Pause Detection:\")\n",
    "print(\"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1ebc2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR Segments: 5\n",
      "Audio-based silences found: 6\n",
      "  Silence 1: 3.70s - 8.73s (5.0s)\n",
      "  Silence 2: 12.05s - 15.16s (3.1s)\n",
      "  Silence 3: 16.16s - 18.82s (2.7s)\n",
      "  Silence 4: 22.61s - 26.17s (3.6s)\n",
      "  Silence 5: 28.75s - 32.36s (3.6s)\n",
      "  Silence 6: 38.44s - 41.34s (2.9s)\n",
      "Added text: 'Hello friends it'sbeenmyfourthday andnow I'vestartedworkingagain onthesameproject.Still theproblems arenotsolvingthatmuch.Finally I'mabletogetmyfile'sdone.'\n",
      "Added pause: 5.0s at 3.70s\n",
      "Added pause: 3.1s at 12.05s\n",
      "Added pause: 2.7s at 16.16s\n",
      "Added pause: 3.6s at 22.61s\n",
      "Added text: '[UM] but still the fuck message now it's still fine.'\n",
      "Added pause: 3.6s at 28.75s\n",
      "Added text: 'That's also better.'\n",
      "Added text: 'Good.'\n",
      "Added pause: 2.9s at 38.44s\n",
      "Added text: 'Good.'\n",
      "\n",
      "Transcript with ALL Pauses Integrated:\n",
      "============================================================\n",
      "Hello friends it'sbeenmyfourthday andnow I'vestartedworkingagain onthesameproject.Still theproblems arenotsolvingthatmuch.Finally I'mabletogetmyfile'sdone. [PAUSE:5.0s] [PAUSE:3.1s] [PAUSE:2.7s] [PAUSE:3.6s] [UM] but still the fuck message now it's still fine. [PAUSE:3.6s] That's also better. Good. [PAUSE:2.9s] Good.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "from pydub import AudioSegment, silence\n",
    "import re\n",
    "\n",
    "def transcribe_with_all_pauses_integrated(audio_file, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Properly integrate ALL detected silences into the transcript\n",
    "    \"\"\"\n",
    "    # Get transcription from CrisperWhisper\n",
    "    model = WhisperModel('nyrahealth/faster_CrisperWhisper', device=\"cpu\", compute_type=\"float32\")\n",
    "    segments, info = model.transcribe(\n",
    "        audio_file, \n",
    "        beam_size=1, \n",
    "        language='en', \n",
    "        word_timestamps=True,\n",
    "        vad_filter=False\n",
    "    )\n",
    "    \n",
    "    segments_list = list(segments)\n",
    "    print(f\"ASR Segments: {len(segments_list)}\")\n",
    "    \n",
    "    # Load audio and detect actual silences\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    silences = silence.detect_silence(\n",
    "        audio,\n",
    "        min_silence_len=int(pause_threshold * 1000),\n",
    "        silence_thresh=-40\n",
    "    )\n",
    "    \n",
    "    print(f\"Audio-based silences found: {len(silences)}\")\n",
    "    for i, (start_ms, end_ms) in enumerate(silences):\n",
    "        duration = (end_ms - start_ms) / 1000\n",
    "        print(f\"  Silence {i+1}: {start_ms/1000:.2f}s - {end_ms/1000:.2f}s ({duration:.1f}s)\")\n",
    "    \n",
    "    # Create timeline events (segments + silences)\n",
    "    events = []\n",
    "    \n",
    "    # Add segment events\n",
    "    for segment in segments_list:\n",
    "        events.append({\n",
    "            'type': 'segment',\n",
    "            'time': segment.start,\n",
    "            'end_time': segment.end,\n",
    "            'text': clean_transcript_text(segment.text)\n",
    "        })\n",
    "    \n",
    "    # Add silence events\n",
    "    for start_ms, end_ms in silences:\n",
    "        start_s = start_ms / 1000\n",
    "        duration = (end_ms - start_ms) / 1000\n",
    "        events.append({\n",
    "            'type': 'silence',\n",
    "            'time': start_s,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    # Sort all events by time\n",
    "    events.sort(key=lambda x: x['time'])\n",
    "    \n",
    "    # Build transcript chronologically\n",
    "    transcript = \"\"\n",
    "    \n",
    "    for event in events:\n",
    "        if event['type'] == 'segment':\n",
    "            transcript += event['text'] + \" \"\n",
    "            print(f\"Added text: '{event['text']}'\")\n",
    "        elif event['type'] == 'silence':\n",
    "            transcript += f\"[PAUSE:{event['duration']:.1f}s] \"\n",
    "            print(f\"Added pause: {event['duration']:.1f}s at {event['time']:.2f}s\")\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "def clean_transcript_text(text):\n",
    "    \"\"\"Clean transcript text\"\"\"\n",
    "    text = re.sub(r',([a-z])', r' \\1', text)\n",
    "    text = re.sub(r',\\s*', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    if text:\n",
    "        text = text[0].upper() + text[1:]\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "result = transcribe_with_all_pauses_integrated(\"converted.wav\", pause_threshold=2.0)\n",
    "print(\"\\nTranscript with ALL Pauses Integrated:\")\n",
    "print(\"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52bd4e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_with_all_pauses_integrated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconverted.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpause_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTranscript with ALL Pauses Integrated:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 19\u001b[0m, in \u001b[0;36mtranscribe_with_all_pauses_integrated\u001b[1;34m(audio_file, pause_threshold)\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m WhisperModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnyrahealth/faster_CrisperWhisper\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, compute_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m segments, info \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(\n\u001b[0;32m     12\u001b[0m     audio_file, \n\u001b[0;32m     13\u001b[0m     beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     vad_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m segments_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msegments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Load audio and detect actual silences\u001b[39;00m\n\u001b[0;32m     23\u001b[0m audio \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_wav(audio_file)\n",
      "File \u001b[1;32mc:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\faster_whisper\\transcribe.py:1187\u001b[0m, in \u001b[0;36mWhisperModel.generate_segments\u001b[1;34m(self, features, tokenizer, options, log_progress, encoder_output)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mlanguage_code \u001b[38;5;241m=\u001b[39m language\n\u001b[0;32m   1174\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prompt(\n\u001b[0;32m   1175\u001b[0m     tokenizer,\n\u001b[0;32m   1176\u001b[0m     previous_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     hotwords\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mhotwords,\n\u001b[0;32m   1180\u001b[0m )\n\u001b[0;32m   1182\u001b[0m (\n\u001b[0;32m   1183\u001b[0m     result,\n\u001b[0;32m   1184\u001b[0m     avg_logprob,\n\u001b[0;32m   1185\u001b[0m     temperature,\n\u001b[0;32m   1186\u001b[0m     compression_ratio,\n\u001b[1;32m-> 1187\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39mno_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     should_skip \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mno_speech_prob \u001b[38;5;241m>\u001b[39m options\u001b[38;5;241m.\u001b[39mno_speech_threshold\n",
      "File \u001b[1;32mc:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\faster_whisper\\transcribe.py:1420\u001b[0m, in \u001b[0;36mWhisperModel.generate_with_fallback\u001b[1;34m(self, encoder_output, prompt, tokenizer, options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1415\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeam_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: options\u001b[38;5;241m.\u001b[39mbeam_size,\n\u001b[0;32m   1417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m\"\u001b[39m: options\u001b[38;5;241m.\u001b[39mpatience,\n\u001b[0;32m   1418\u001b[0m     }\n\u001b[1;32m-> 1420\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_no_speech_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuppress_blank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppress_blank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuppress_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppress_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_initial_timestamp_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_initial_timestamp_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1435\u001b[0m tokens \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msequences_ids[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;66;03m# Recover the average log prob from the returned score.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "from pydub import AudioSegment, silence\n",
    "import re\n",
    "\n",
    "def transcribe_with_all_pauses_integrated(audio_file, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Properly integrate ALL detected silences into the transcript\n",
    "    \"\"\"\n",
    "    # Get transcription from CrisperWhisper\n",
    "    model = WhisperModel('nyrahealth/faster_CrisperWhisper', device=\"cpu\", compute_type=\"float32\")\n",
    "    segments, info = model.transcribe(\n",
    "        audio_file, \n",
    "        beam_size=1, \n",
    "        language='en', \n",
    "        word_timestamps=True,\n",
    "        vad_filter=False\n",
    "    )\n",
    "    \n",
    "    segments_list = list(segments)\n",
    " \n",
    "    \n",
    "    # Load audio and detect actual silences\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    silences = silence.detect_silence(\n",
    "        audio,\n",
    "        min_silence_len=int(pause_threshold * 1000),\n",
    "        silence_thresh=-40\n",
    "    )\n",
    "    \n",
    "   \n",
    "    for i, (start_ms, end_ms) in enumerate(silences):\n",
    "        duration = (end_ms - start_ms) / 1000\n",
    "   \n",
    "    \n",
    "    # Create timeline events (segments + silences)\n",
    "    events = []\n",
    "    \n",
    "    # Add segment events\n",
    "    for segment in segments_list:\n",
    "        events.append({\n",
    "            'type': 'segment',\n",
    "            'time': segment.start,\n",
    "            'end_time': segment.end,\n",
    "            'text': clean_transcript_text(segment.text)\n",
    "        })\n",
    "    \n",
    "    # Add silence events\n",
    "    for start_ms, end_ms in silences:\n",
    "        start_s = start_ms / 1000\n",
    "        duration = (end_ms - start_ms) / 1000\n",
    "        events.append({\n",
    "            'type': 'silence',\n",
    "            'time': start_s,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    # Sort all events by time\n",
    "    events.sort(key=lambda x: x['time'])\n",
    "    \n",
    "    # Build transcript chronologically\n",
    "    transcript = \"\"\n",
    "    \n",
    "    for event in events:\n",
    "        if event['type'] == 'segment':\n",
    "            transcript += event['text'] + \" \"\n",
    "           \n",
    "        elif event['type'] == 'silence':\n",
    "            transcript += f\"[PAUSE:{event['duration']:.1f}s] \"\n",
    "          \n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "def clean_transcript_text(text):\n",
    "    \"\"\"Clean transcript text\"\"\"\n",
    "    text = re.sub(r',([a-z])', r' \\1', text)\n",
    "    text = re.sub(r',\\s*', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    if text:\n",
    "        text = text[0].upper() + text[1:]\n",
    "    return text\n",
    "\n",
    "# Usage\n",
    "result = transcribe_with_all_pauses_integrated(\"converted.wav\", pause_threshold=2.0)\n",
    "print(\"\\nTranscript with ALL Pauses Integrated:\")\n",
    "print(\"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c2ebe",
   "metadata": {},
   "source": [
    "## format sahi kardu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b889bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path \n",
    "\n",
    "def convert_to_wav(input_file, output_file=None):\n",
    "    \"\"\"Convert audio file to WAV format using ffmpeg\"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = input_file.replace(Path(input_file).suffix, '_converted.wav')\n",
    "    \n",
    "    try:\n",
    "        # Use ffmpeg to convert the file\n",
    "        subprocess.run([\n",
    "            'ffmpeg', '-i', input_file, \n",
    "            '-acodec', 'pcm_s16le', \n",
    "            '-ar', '16000', \n",
    "            '-ac', '1',  # mono\n",
    "            '-y',  # overwrite output file\n",
    "            output_file\n",
    "        ], check=True, capture_output=True)\n",
    "        \n",
    "        print(f\"Successfully converted to: {output_file}\")\n",
    "        return output_file\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"ffmpeg conversion failed: {e}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(\"ffmpeg not found. Please install ffmpeg.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a91e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted to: question_1_1757186277869_converted.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'question_1_1757186277869_converted.wav'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_wav(\"question_1_1757186277869.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f70172",
   "metadata": {},
   "source": [
    "## Webm to WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e48bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted question_0_1757221339361.webm to converted.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'converted.wav'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def convert_webm_to_wav_subprocess(input_file, output_file=\"output.wav\"):\n",
    "    \"\"\"Convert WebM to WAV using subprocess\"\"\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\", \"-i\", input_file,\n",
    "            \"-acodec\", \"pcm_s16le\",\n",
    "            \"-ar\", \"16000\", \n",
    "            \"-ac\", \"1\",\n",
    "            \"-y\",  # overwrite output\n",
    "            output_file\n",
    "        ], check=True, capture_output=True)\n",
    "        \n",
    "        print(f\"Successfully converted {input_file} to {output_file}\")\n",
    "        return output_file\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"FFmpeg error: {e}\")\n",
    "        raise\n",
    "    except FileNotFoundError:\n",
    "        print(\"FFmpeg not found. Please install FFmpeg binary.\")\n",
    "        raise\n",
    "\n",
    "# Usage\n",
    "convert_webm_to_wav_subprocess(\"question_0_1757221339361.webm\", \"converted.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9a65d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "print(dir(ffmpeg))  # Should show 'input' in the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41be831",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ffmpeg' has no attribute 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mffmpeg\u001b[39;00m\n\u001b[0;32m      3\u001b[0m (\n\u001b[0;32m      4\u001b[0m     \u001b[43mffmpeg\u001b[49m\n\u001b[1;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_0_1757221339361.webm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moutput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, acodec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcm_s16le\u001b[39m\u001b[38;5;124m\"\u001b[39m, ar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, ac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ffmpeg' has no attribute 'input'"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "(\n",
    "    ffmpeg\n",
    "    .input(\"question_0_1757221339361.webm\")\n",
    "    .output(\"output.wav\", acodec=\"pcm_s16le\", ar=16000, ac=1)\n",
    "    .run()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc53fdc",
   "metadata": {},
   "source": [
    "## Audio analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145f5d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\Pranav Abani\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Recording (6).wav...\n",
      "   Converting 48000Hz  16kHz\n",
      "   Converted to mono\n",
      "   Saved: Recording (6)_fixed.wav\n",
      "ASR Segments: 0\n",
      "No segments detected - trying with different parameters...\n",
      "Transcription failed: WhisperModel.transcribe() got an unexpected keyword argument 'logprob_threshold'. Did you mean 'log_prob_threshold'?\n",
      "\n",
      "Fixed Transcript:\n",
      "============================================================\n",
      "Transcription failed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "from pydub import AudioSegment, silence\n",
    "import re\n",
    "import os\n",
    "\n",
    "def fix_audio_for_transcription(audio_file):\n",
    "    \"\"\"\n",
    "    Fix the specific issues found in your audio files\n",
    "    \"\"\"\n",
    "    print(f\"Preprocessing {audio_file}...\")\n",
    "    \n",
    "    # Load audio\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    \n",
    "    # Fix sample rate (your main issue)\n",
    "    if audio.frame_rate != 16000:\n",
    "        print(f\"   Converting {audio.frame_rate}Hz  16kHz\")\n",
    "        audio = audio.set_frame_rate(16000)\n",
    "    \n",
    "    # Fix clipping issues (your second issue)\n",
    "    if audio.max_dBFS > -6:\n",
    "        reduction = audio.max_dBFS + 12  # Target -12dBFS\n",
    "        audio = audio - reduction\n",
    "        print(f\"   Reduced clipping: -{reduction:.1f}dB\")\n",
    "    \n",
    "    # Ensure mono\n",
    "    if audio.channels > 1:\n",
    "        audio = audio.set_channels(1)\n",
    "        print(\"   Converted to mono\")\n",
    "    \n",
    "    # Save fixed audio\n",
    "    base_name = os.path.splitext(audio_file)[0]\n",
    "    fixed_file = f\"{base_name}_fixed.wav\"\n",
    "    audio.export(fixed_file, format=\"wav\")\n",
    "    print(f\"   Saved: {fixed_file}\")\n",
    "    \n",
    "    return fixed_file\n",
    "\n",
    "def transcribe_with_audio_fixes(audio_file, pause_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Your existing transcription function with audio preprocessing\n",
    "    \"\"\"\n",
    "    # Fix audio issues first\n",
    "    try:\n",
    "        fixed_file = fix_audio_for_transcription(audio_file)\n",
    "        use_file = fixed_file\n",
    "    except Exception as e:\n",
    "        print(f\"Preprocessing failed: {e}, using original file\")\n",
    "        use_file = audio_file\n",
    "    \n",
    "    try:\n",
    "        # Get transcription from CrisperWhisper\n",
    "        model = WhisperModel('nyrahealth/faster_CrisperWhisper', device=\"cpu\", compute_type=\"float32\")\n",
    "        segments, info = model.transcribe(\n",
    "            use_file, \n",
    "            beam_size=3,  # Higher beam size for better quality\n",
    "            language='en', \n",
    "            word_timestamps=True,\n",
    "            vad_filter=False,\n",
    "            temperature=0.0,  # More deterministic\n",
    "            condition_on_previous_text=False  # Prevent context bias\n",
    "        )\n",
    "        \n",
    "        segments_list = list(segments)\n",
    "        print(f\"ASR Segments: {len(segments_list)}\")\n",
    "        \n",
    "        if len(segments_list) == 0:\n",
    "            print(\"No segments detected - trying with different parameters...\")\n",
    "            # Try again with more permissive settings\n",
    "            segments, info = model.transcribe(\n",
    "                use_file,\n",
    "                beam_size=1,\n",
    "                language='en',\n",
    "                word_timestamps=True,\n",
    "                vad_filter=False,\n",
    "                compression_ratio_threshold=1.8,\n",
    "                logprob_threshold=-1.5\n",
    "            )\n",
    "            segments_list = list(segments)\n",
    "        \n",
    "        # Load audio and detect actual silences\n",
    "        audio = AudioSegment.from_wav(use_file)\n",
    "        silences = silence.detect_silence(\n",
    "            audio,\n",
    "            min_silence_len=int(pause_threshold * 1000),\n",
    "            silence_thresh=-35  # Slightly more sensitive for processed audio\n",
    "        )\n",
    "        \n",
    "        print(f\"Audio-based silences found: {len(silences)}\")\n",
    "        for i, (start_ms, end_ms) in enumerate(silences):\n",
    "            duration = (end_ms - start_ms) / 1000\n",
    "            print(f\"  Silence {i+1}: {start_ms/1000:.2f}s - {end_ms/1000:.2f}s ({duration:.1f}s)\")\n",
    "        \n",
    "        # Create timeline events (segments + silences)\n",
    "        events = []\n",
    "        \n",
    "        # Add segment events\n",
    "        for segment in segments_list:\n",
    "            events.append({\n",
    "                'type': 'segment',\n",
    "                'time': segment.start,\n",
    "                'end_time': segment.end,\n",
    "                'text': clean_transcript_text(segment.text)\n",
    "            })\n",
    "        \n",
    "        # Add silence events\n",
    "        for start_ms, end_ms in silences:\n",
    "            start_s = start_ms / 1000\n",
    "            duration = (end_ms - start_ms) / 1000\n",
    "            events.append({\n",
    "                'type': 'silence',\n",
    "                'time': start_s,\n",
    "                'duration': duration\n",
    "            })\n",
    "        \n",
    "        # Sort all events by time\n",
    "        events.sort(key=lambda x: x['time'])\n",
    "        \n",
    "        # Build transcript chronologically\n",
    "        transcript = \"\"\n",
    "        \n",
    "        for event in events:\n",
    "            if event['type'] == 'segment':\n",
    "                transcript += event['text'] + \" \"\n",
    "                print(f\"Added text: '{event['text']}'\")\n",
    "            elif event['type'] == 'silence':\n",
    "                transcript += f\"[PAUSE:{event['duration']:.1f}s] \"\n",
    "                print(f\"Added pause: {event['duration']:.1f}s at {event['time']:.2f}s\")\n",
    "        \n",
    "        # Cleanup temp file\n",
    "        if use_file != audio_file and os.path.exists(use_file):\n",
    "            os.remove(use_file)\n",
    "        \n",
    "        return transcript.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Transcription failed: {e}\")\n",
    "        return \"Transcription failed\"\n",
    "\n",
    "def clean_transcript_text(text):\n",
    "    \"\"\"Clean transcript text\"\"\"\n",
    "    text = re.sub(r',([a-z])', r' \\1', text)\n",
    "    text = re.sub(r',\\s*', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    if text:\n",
    "        text = text[0].upper() + text[1:]\n",
    "    return text\n",
    "\n",
    "# Usage - this should now work consistently\n",
    "result = transcribe_with_audio_fixes(\"Recording (6).wav\", pause_threshold=2.0)\n",
    "print(\"\\nFixed Transcript:\")\n",
    "print(\"=\"*60)\n",
    "print(result)\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84218da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
